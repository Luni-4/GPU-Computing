\documentclass[
   %handout
 ]{beamer}
 
\usetheme{simple}
\usepackage{lmodern}
\usepackage[scale=2]{ccicons}

%Codifica dei font di input e di output
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% Converte file eps in pdf
\usepackage{epstopdf}

% Consente di impostare le virgolette del discorso diretto
\usepackage{dirtytalk}

% Consente di impostare i commenti multiline
\usepackage{verbatim}

% Consente di disegnare grafi
\usepackage{tikz}

% Libreria per disegnare cerchi e frecce di tikz
\usetikzlibrary{arrows}

% Definizione della cartella contenente le immagini da usare
\graphicspath{{img/}}

% Disabilitare la trasparenza sulla pause
\setbeamercovered{invisible}
            
% TODO
% Finire Parte algoritmo di apprendimento
% Backpropagation
% Migliorare la spiegazione delle slide


% Watermark background (simple theme)
%\setwatermark{\includegraphics[height=8cm]{img/Heckert_GNU_white.png}}
     
%\institute{\url{http://github.com/famuvie}}

% Metadati di apertura del pdf
\hypersetup{
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdfencoding=auto
}

% Definizione degli autori
\author {
            \texorpdfstring{\hspace*{0.01em}{\Large Michele Valsesia}}{Michele Valsesia} 
            \texorpdfstring{\\ \bigskip}{e}
            \texorpdfstring{\hspace*{0.3em}{\Large Nicholas Aspes }}{Nicholas Aspes}
        }

\begin{document}

% Definizione del Titolo e Anno Accademico

\title{Implementazione di una \\ 
       Rete Convoluzionale in CUDA \bigskip}
        
\date{\Large Anno accademico 2018/2019}


% Titolo della Presentazione
     
\begin{frame}
\maketitle
\end{frame}

% //////////////////////////////// Introduzione ////////////////////////////////////////


\begin{frame}{Introduzione}
    \framesubtitle{Obiettivi}  
    
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large Descrivere brevemente l'architettura ed il funzionamento di una \emph{Rete Neurale}
        \item \large Motivare le differenti scelte implementative adottate durante lo svolgimento del progetto
        \item \large Valutare l'accuratezza e lo speed-up della rete rispetto ad una sua implementazione sequenziale       
    \end{itemize}  
\end{frame}


% //////////////////////////////// Parte Teorica Reti Neurali //////////////////////////

\begin{frame}[c]
  \centering
  \bigskip \bigskip    
  \Huge Reti Neurali
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Scopo}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Le \emph{Reti Neurali} vengono principalmente usate per la classificazione di immagini
       \item \large Il processo di classificazione consiste nell'assegnare ad un immagine un'etichetta che identifichi nel miglior modo possibile il suo contenuto semantico
       \item \large L'insieme delle immagini che hanno tutte la stessa etichetta costituiscono una \emph{classe}
       \item \large Le reti neurali ricevono in input un'immagine e forniscono in output la relativa classe 
    \end{itemize}
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Funzionamento}
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
        \item \large Una rete neurale deve \emph{apprendere} come assegnare correttamente le immagini alle varie classi
        \item \large Un \emph{esempio} è una coppia (immagine, etichetta)
        \item \large Un team di persone valuta il contenuto semantico di ciascuna immagine e assegna all'esempio l'etichetta corrispondente
        \item \large Il \emph{training set} ed il \emph{test set} sono insiemi di esempi
        \item \large Il training set viene usato per l'addestramento (training) della rete
        \item \large Il test set serve a controllare che la rete abbia imparato a discriminare correttamente le immagini
       
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Training}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Per ognuno degli esempi del training set
        
        \bigskip
        \bigskip
        
        \setbeamertemplate{itemize items}[square] 
        \begin{itemize} 
        \setlength\itemsep{3em}
            \item \large La rete riceve in input l'immagine relativa all'esempio considerato e l'associa ad una delle classi presenti
            \item \large Se la classe in output è diversa dall'etichetta dell'esempio, la rete corregge i suoi parametri interni e passa all'immagine successiva
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Testing}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large L'\emph{accuratezza} della rete è data dal rapporto tra il numero di esempi classificati scorrettamente ed il numero totale di esempi
        
        \item \large Per ognuno degli esempi del test set
        
        \bigskip
        
        \setbeamertemplate{itemize items}[square] 
        \begin{itemize} 
        \setlength\itemsep{2em}
            \item \large La rete riceve in input l'immagine dell'esempio considerato e l'associa ad una delle classi presenti
            \item \large Ogni volta che l'output della rete non corrisponde all'etichetta dell'esempio viene incrementato un contatore, necessario per il calcolo dell'accuratezza
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Significato Biologico}
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large Le \emph{Reti Neurali} nascono con lo scopo di modellare una rete neurale biologica
       \item \large Una rete neurale biologica si compone di unità cellulari di base: i \emph{neuroni}
       \item \large I neuroni sono collegati tra loro per mezzo di specifiche giunture chiamate \emph{sinapsi}
    \end{itemize}
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Neurone}
    
    \begin{center}
      \includegraphics[scale = 0.35]{neuron_model.jpeg}
    \end{center}
  
    \bigskip 
  
  \begin{itemize}
    \setlength\itemsep{1em}
    \item[] \large \emph{Modello matematico di un neurone}
  \end{itemize}       
\end{frame} 


\begin{frame}{Reti Neurali}
    \framesubtitle{Funzionamento Neurone}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Attraverso un meccanismo di eccitazione ed inibizione i pesi sinaptici controllano quanto un neurone sia influenzato dagli altri
       \item \large I segnali in ingresso al neurone vengono pesati dalle differenti sinapsi, trasportati dai dendriti all'interno del corpo cellulare e sommati tra loro
       \item \large Quando la somma supera una certa soglia, il neurone \emph{spara} un segnale lungo l'assone 
       \item \large La \emph{frequenza di sparo} del neurone viene modellata con una funzione di attivazione $f$       
    \end{itemize}
\end{frame}


\begin{frame}{Reti Neurali}
    \framesubtitle{Funzioni di Attivazione}
    \begin{block}{Definizione} 
        \large Una \emph{funzione di attivazione} è una funzione matematica non lineare usata per calcolare l'output di un neurone. Il suo input equivale alla somma pesata dei segnali in ingresso al neurone
    \end{block}\pause
    
    \bigskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \emph{\large Sigmoide}
        \item \emph{\large Tangente Iperbolica}
        \item \emph{\large Softplus}
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Sigmoide}
    \begin{block}{Definizione} 
        \large La \emph{Sigmoide} $\sigma: \mathbb{R} \rightarrow [0, 1]$ è definita come $\sigma(x) = \frac{1}{(1 + e^{-x})}$ 
    \end{block}\pause
    
    \bigskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Per elevati valori negativi di input la sigmoide restituisce 0: il neurone non spara affatto
        \item \large Per elevati valori positivi la sigmoide restituisce 1: il neurone satura e spara con frequenza di sparo pari a 1
        \item \large La sua derivata è uguale a $\sigma^{\prime}(x) = 1 - \sigma(x)$
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Sigmoide}
    
    \begin{center}
      \includegraphics[scale = 0.6]{sigmoid.jpeg}
    \end{center}
  
    \bigskip 
  
  \begin{itemize}
    \setlength\itemsep{1em}
    \item[] \large \emph{Rappresentazione grafica Sigmoide}
  \end{itemize}       
\end{frame} 



\begin{frame}{Reti Neurali}
    \framesubtitle{Tangente Iperbolica}
    \begin{block}{Definizione} 
        \large La \emph{Tangente Iperbolica} $\tanh: \mathbb{R} \rightarrow [-1, 1]$ è definita come $\tanh(x) = 2\sigma(2x) - 1$ 
    \end{block}\pause
    
    \bigskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large La tangente iperbolica è una sigmoide scalata 
        \item \large La sua derivata è uguale a $\tanh^{\prime}(x) = 1 - \tanh^{2}(x)$
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Tangente Iperbolica}
    
    \begin{center}
      \includegraphics[scale = 0.6]{tanh.jpeg}
    \end{center}
  
    \bigskip 
  
  \begin{itemize}
    \setlength\itemsep{1em}
    \item[] \large \emph{Rappresentazione grafica Tangente Iperbolica}
  \end{itemize}       
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Softplus}
    \begin{block}{Definizione} 
        \large La \emph{Softplus} $s: \mathbb{R} \rightarrow [0, +\infty]$ è definita come $s(x) = \log(1 + e^x)$ 
    \end{block}\pause
    
    \bigskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large La softplus è un approssimazione della \emph{Rectifier Linear Unit (ReLU)}
        \item \large È solitamente usata per sostituire la ReLU che presenta un punto di discontinuità in 0
        \item \large La sua derivata è uguale a $s^{\prime}(x) = \frac{1}{(1 + e^{-x})}$
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Softplus}
    
    \begin{center}
      \includegraphics[scale = 0.45]{softplus_vs_rectifier.png}
    \end{center}
  
    \smallskip
  
  \begin{itemize}
    \setlength\itemsep{1em}
    \item[] \large \emph{Confronto grafico tra ReLU e Softplus}
  \end{itemize}       
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Rete Neurale}
    
    \begin{block}{Definizione} 
        \large Una \emph{Rete Neurale} è composta da un insieme di neuroni connessi tra loro in un grafo aciclico
    \end{block}\pause
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large  I neuroni sono organizzati in insiemi distinti chiamati \emph{livelli} o \emph{layer}
        \item \large I livelli vengono posti uno di seguito all'altro in modo da formare una sequenza
        \item \large I livelli intermedi prendono il nome di \emph{hidden}
        \item \large L'output dei neuroni di un livello diventano l'input dei neuroni del livello successivo        
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Rete Neurale}
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Quando si effettua il conteggio dei livelli di una rete non si considera il livello di input
        \item \large Una rete a \emph{singolo livello} non presenta livelli hidden    
        \item \large Per determinare la grandezza di una rete ci si concentra sul numero di neuroni e sui relativi pesi ad essi associati
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Livello Fully-Connected}
    
    \begin{block}{Definizione} 
        \large Un livello è di tipo \emph{Fully-Connected} quando i neuroni appartenenti a due livelli adiacenti sono completamente connessi tra loro mentre i neuroni associati ad un singolo livello non condividono nessuna connessione 
    \end{block}\pause
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large I pesi dei neuroni di un livello vengono salvati all'interno di matrici
        \item \large Le righe della matrice identificano i neuroni del livello mentre le colonne rappresentano i pesi di ciascun neurone
        \item \large La struttura a livelli di una rete neurale consente di facilitare le varie operazioni sfruttando il calcolo matriciale 
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Livello Fully-Connected}
    
    \begin{center}
      \includegraphics[scale = 0.4]{fully_connected.jpeg}
    \end{center}
  
    \smallskip
  
    \begin{itemize}
        \setlength\itemsep{1em}
        \item[] \large \emph{Una rete neurale a 3 livelli}
    \end{itemize}       
\end{frame} 



\begin{frame}{Reti Neurali}
    \framesubtitle{Funzionamento}
    
    Il processo di apprendimento di una rete neurale è suddiviso in quattro fasi distinte \pause
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \emph{\large Inizializzazione dei pesi}
        \item \emph{\large Forward Propagation}
        \item \emph{\large Funzione di perdita}
        \item \emph{\large Back Propagation}
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Inizializzazione dei pesi}
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Al momento della nascita gli esseri umani non sono in grado di discriminare nessun tipo di oggetto a causa del mancato addestramento della loro rete neurale biologica
        \item \large Per riprodurre questo comportamento, all'inizio della fase di training, i pesi sinaptici $w_i$ di ciascun livello vengono inizializzati in maniera casuale     
    \end{itemize}
\end{frame}



\begin{frame}{Reti Neurali}
    \framesubtitle{Forward Propagation}
    
    \begin{block}{Definizione} 
        \large La \emph{Forward Propagation} è il meccanismo utilizzato da una rete neurale per associare un'immagine ad una determinata classe
    \end{block}\pause
        
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large L'output dei neuroni del livello $i$ viene moltiplicato per la matrice dei pesi del livello $i+1$ ottenendo il vettore $v$
        \item \large Al vettore $v$ viene aggiunto il vettore dei bias del livello $i+1$
        \item \large L'output del livello $i+1$ si ottiene applicando la funzione di attivazione $f$ ad ogni entry del vettore $v$
        \item \large Le operazioni precedenti sono svolte per tutti i livelli ad eccezione dell'ultimo
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Funzione di perdita}
    
    \begin{block}{Definizione} 
        \large Una \emph{funzione di perdita} $L$ viene utilizzata per determinare l'errore di classificazione di una rete neurale
    \end{block}\pause
        
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
        \item \large La funzione di perdita più usata è la \emph{Mean Squared Error (MSE)} $L = \frac{1}{2} \sum (y - o)^{2}$
        \item \large $y$ identifica l'output della rete mentre $o$ l'etichetta dell'esempio considerato
        \item \large Minimizzando la funzione di perdita $L$ si riduce l'errore di una rete neurale
        \item \large Zalcolando la derivata di $L$ in funzione dei pesi $w_i$ si individua il minimo globale della funzione di perdita
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Funzione di perdita}
    
    \begin{center}
      \includegraphics[scale = 0.7]{Loss.png}
    \end{center}
  
    \smallskip
  
    \begin{itemize}
        \setlength\itemsep{1em}
        \item[] \large \emph{Mean Squared Error (MSE). I pesi $w_1$ e $w_2$ sono le variabili indipendenti. La funzione di perdita $L$ è la variabile dipendente}
    \end{itemize}       
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Back Propagation}
    
    \begin{block}{Definizione} 
        \large La \emph{Back Propagation} è il meccanismo utilizzato da una rete neurale per correggere gli errori di classificazione. Si individuano i pesi $w_i$ che hanno influenzato maggiormente l'errore commesso e si aggiusta il loro valore in modo da ridurre la funzione di perdita
    \end{block}\pause
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Per calcolare la derivata della funzione $L$ in funzione dei pesi $w_i$ viene usata la \emph{regola della catena (chain rule)}
        \item \large Questa regola viene usata per trovare la derivata di una funzione composta
    \end{itemize}      
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Aggiornamento dei Pesi e Learning Rate}
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Il nuovo valore del peso $w_i$ è dato da  $w_i = w_i - \eta\frac{\partial L}{\partial w_i} = w_i + \Delta w_i$
        \item \large Il \emph{learning rate} $\eta$ è un parametro usato per controllare la velocità aggiornamento dei pesi. 
        \item \large Un learning rate alto comporta aggiornamenti rapidi, un tempo di esecuzione più basso, ma una maggiore probabilità di terminare in un minimo locale
        \item \large Al contrario, un basso learning rate diminuisce la probabilità di terminare in un minimo locale, ma i tempi di esecuzione si allungano notevolmente
    \end{itemize}      
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    
    \begin{center}
      \includegraphics[scale = 0.35]{back1.png}
    \end{center}
    
    \begin{columns}
        \begin{column}{0.48\textwidth}
            \begin{equation*}
                z_j^{h} = \sum_{i=0}^{n} w_{ij}^{h}x_i
            \end{equation*}
        \end{column}
        \begin{column}{0.48\textwidth}
            \begin{equation*}
                z^{o} = \sum_{j=0}^{m} w_{j}^{o}h_j
            \end{equation*}
        \end{column}
    \end{columns}
    
    \smallskip
    
    \begin{columns}
        \begin{column}{0.48\textwidth}
            \begin{equation*}
                h_j = f(z_j^{h})
            \end{equation*}
        \end{column}
        \begin{column}{0.48\textwidth}
            \begin{equation*}
                o = f(z^{o})
            \end{equation*}
        \end{column}
    \end{columns}    
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Dato il neurone $o$ appartenente all'ultimo livello della rete neurale
        \begin{center}
         \large $ \frac{\partial L}{\partial w_j^{o}} =  \frac{\partial L}{\partial o} \cdot \frac{\partial o}{\partial z^{o}} \cdot \frac{\partial z^{o}}{\partial w_j}$         
         \end{center}
         
         \setbeamertemplate{itemize items}[square]
         \begin{itemize}
            \setlength\itemsep{2em}
            \item \large $\frac{\partial L}{\partial o} = \frac{\partial}{\partial o} [ \frac{1}{2}(y - o)^{2} ] = -(y - o)$
            \item \large $\frac{\partial o}{\partial z^{o}} = f^{\prime}(z^{o})$
            \item \large $\frac{\partial z^{o}}{\partial w_j^{o}} = h_j$ 
         \end{itemize}
    \end{itemize}
\end{frame}


\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Derivata della funzione $L$ in funzione del peso $w_j^{o}$ 
        \begin{center}
         \large $ \frac{\partial L}{\partial w_j^{o}} =  -(y - o) \cdot f^{\prime}(z^{o}) \cdot h_j = -\delta_j^{o}h_j$         
         \end{center}
         \item \large Aggiornamento del peso $w_j^{o}$
         \begin{center}
         \large $ \Delta w_j^{o} = \eta\delta_j^{o}h_j$         
         \end{center}       
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    
    \begin{center}
      \includegraphics[scale = 0.4]{back2.png}
    \end{center}
    
    \bigskip  

     \begin{equation*}
         \frac{\partial L}{\partial w_{ij}^{h}} =  \frac{\partial L}{\partial o} \cdot \frac{\partial o}{\partial z^{o}} \cdot \frac{\partial z^{o}}{\partial h_j} \cdot \frac{\partial h_j}{\partial z_j^{h}} \cdot \frac{\partial z_j^{h}}{\partial w_{ij}^{h}}
     \end{equation*}            
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        
        \item[] \large \begin{center} $\frac{\partial L}{\partial o} = \frac{\partial}{\partial o} [ \frac{1}{2}(y - o)^{2} ] = -(y - o)$ \end{center}
        \item[] \large \begin{center} $\frac{\partial o}{\partial z^{o}} = f^{\prime}(z^{o})$ \end{center}
        \item[] \large \begin{center} $\frac{\partial z^{o}}{\partial h_j} = w_j^{o}$ \end{center}
        \item[] \large \begin{center} $\frac{\partial h_j}{\partial z_j^{h}} = f^{\prime}(z_j^{h})$ \end{center}
        \item[] \large \begin{center} $\frac{\partial z_j^{h}}{\partial w_{ij}^{h}} = x_i$ \end{center}          
    \end{itemize}
\end{frame}


\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Derivata della funzione $L$ in funzione del peso $w_{ij}^{h}$ 
        \begin{center}
         \large $ \frac{\partial L}{\partial w_{ij}^{h}} =  -(y - o) \cdot f^{\prime}(z^{o}) \cdot w_j^{o} \cdot f^{\prime}(z_j^{h}) \cdot x_i = -\delta_j^{h}x_i$         
         \end{center}
         \item \large Aggiornamento del peso $w_{ij}^{h}$
         \begin{center}
         \large $ \Delta w_{ij}^{h} = \eta\delta_j^{h}x_i$         
         \end{center}       
    \end{itemize}
\end{frame} 






\begin{frame}{Reti Neurali}
    \framesubtitle{Rete Neurale Convoluzionale}  
    Una \emph{Rete Neurale Convoluzionale} si differenzia da una più classica in quanto assume che l'input della rete sia un'immagine
\end{frame}   

\begin{frame}[c]
  \centering
  \bigskip \bigskip    
  \Huge Implementazione della Rete
\end{frame}

\begin{frame}[c]
  \centering
  \bigskip \bigskip    
  \Huge Analisi dei Risultati
\end{frame}


\end{document}
