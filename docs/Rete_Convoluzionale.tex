\documentclass[
   %handout
 ]{beamer}
 
\usetheme{simple}
\usepackage{lmodern}
\usepackage[scale=2]{ccicons}

%Codifica dei font di input e di output
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% Converte file eps in pdf
\usepackage{epstopdf}

% Consente di rappresentare i simboli di check e di cross
\usepackage{pifont}

% Consente di suddividere una colonna in due righe
\usepackage{multirow}

% Consente di rappresentare la funzione identità
\usepackage{dsfont}

% Consente di impostare le virgolette del discorso diretto
%\usepackage{dirtytalk}

% Consente di impostare i commenti multiline
\usepackage{verbatim}

% Consente di disegnare grafi
\usepackage{tikz}

% Libreria per disegnare cerchi e frecce di tikz
\usetikzlibrary{arrows}

% Definizione della cartella contenente le immagini da usare
\graphicspath{{img/}}

% Disabilitare la trasparenza sulla pause
\setbeamercovered{invisible}

%TODO
% Definire i confronti tra le configurazioni e le accuratezze ottenute (bisogna aspettare di fare i test)
% Modificare le conclusioni in base ai risultati ottenuti con i test          


% Watermark background (simple theme)
%\setwatermark{\includegraphics[height=8cm]{img/Heckert_GNU_white.png}}
     
%\institute{\url{http://github.com/famuvie}}

% Metadati di apertura del pdf
\hypersetup{
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdfencoding=auto
}

% Definizione degli autori
\author {
            \texorpdfstring{\hspace*{0.01em}{\Large Michele Valsesia}}{Michele Valsesia} 
            \texorpdfstring{\\ \bigskip}{e}
            \texorpdfstring{\hspace*{0.3em}{\Large Nicholas Aspes }}{Nicholas Aspes}
        }

\begin{document}

% Definizione del Titolo e Anno Accademico

\title{Implementazione di una \\ 
       Rete Convoluzionale in CUDA \bigskip}
        
\date{\Large Anno accademico 2018/2019}


% Titolo della Presentazione
     
\begin{frame}
\maketitle
\end{frame}

% //////////////////////////////// Introduzione ////////////////////////////////////////


\begin{frame}{Introduzione}
    \framesubtitle{Obiettivi}  
    
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large Descrivere l'architettura ed il funzionamento di una \emph{Rete Neurale Semplice} e di una \emph{Convoluzionale}
        \item \large Motivare le differenti scelte implementative adottate durante lo svolgimento del progetto
        \item \large Valutare l'accuratezza e lo speedup della rete rispetto ad una implementazione di tipo sequenziale       
    \end{itemize}  
\end{frame}


% //////////////////////////////// Parte Teorica Reti Neurali //////////////////////////

\begin{frame}[c]
  \centering
  \bigskip \bigskip    
  \Huge Reti Neurali
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Scopo}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Le \emph{Reti Neurali} vengono principalmente usate per la classificazione di immagini
       \item \large Il processo di classificazione consiste nell'assegnare ad un immagine un'etichetta che identifichi nel miglior modo possibile il suo contenuto semantico
       \item \large Un'etichetta è meglio conosciuta con il nome di \emph{classe}
       \item \large Le reti neurali ricevono in input un'immagine e restituiscono in output la relativa classe 
    \end{itemize}
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Funzionamento}
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
        \item \large Una rete neurale deve \emph{apprendere} come assegnare correttamente alle immagini le varie classi
        \item \large Un \emph{esempio} è una coppia (immagine, etichetta)
        \item \large Un esempio viene creato da un team di persone che valuta il contenuto semantico di un immagine e le associa l'etichetta più adatta 
        \item \large Il \emph{training set} ed il \emph{test set} sono insiemi di esempi
        \item \large Il training set viene usato per l'addestramento (training) della rete
        \item \large Il test set serve a controllare che la rete abbia imparato a discriminare correttamente le immagini       
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Training}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Per ognuno degli esempi del training set
        
        \bigskip
        \bigskip
        
        \setbeamertemplate{itemize items}[square] 
        \begin{itemize} 
        \setlength\itemsep{3em}
            \item \large La rete assegna all'immagine corrente la classe che meglio rappresenta il suo contenuto semantico
            \item \large Se la classe di output è diversa dall'etichetta dell'esempio, la rete corregge i suoi parametri interni e passa all'immagine successiva
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Testing}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large L'\emph{accuratezza} della rete è data dal rapporto tra il numero di esempi classificati correttamente e la cardinalità del test set
        
        \item \large Per ognuno degli esempi del test set
        
        \bigskip
        
        \setbeamertemplate{itemize items}[square] 
        \begin{itemize} 
        \setlength\itemsep{2em}
           \item \large La rete assegna all'immagine corrente la classe che meglio rappresenta il suo contenuto semantico
            \item \large Per sapere il numero di immagini classificate correttamente dalla rete è necessario definire un contatore
            \item \large Il contatore viene incrementato quando l'output prodotto è uguale all'etichetta dell'esempio considerato
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Significato Biologico}
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large Le \emph{Reti Neurali} nascono con lo scopo di modellare una rete neurale biologica
       \item \large Una rete neurale biologica si compone di unità cellulari di base: i \emph{neuroni}
       \item \large I neuroni sono collegati tra loro per mezzo di specifiche giunture chiamate \emph{sinapsi}
    \end{itemize}
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Neurone}
    
    \begin{center}
      \includegraphics[scale = 0.35]{neuron_model.jpeg}
    \end{center}
  
    \bigskip 
  
  \begin{itemize}
    \setlength\itemsep{1em}
    \item[] \large \emph{Modello matematico di un neurone}
  \end{itemize}       
\end{frame} 


\begin{frame}{Reti Neurali}
    \framesubtitle{Funzionamento Neurone}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Attraverso un meccanismo di eccitazione ed inibizione i pesi sinaptici controllano quanto un neurone sia influenzato dagli altri
       \item \large I segnali in ingresso al neurone vengono pesati dalle differenti sinapsi, trasportati dai dendriti all'interno del corpo cellulare e sommati tra loro
       \item \large Quando la somma supera una certa soglia, il neurone \emph{spara} un segnale lungo l'assone 
       \item \large La \emph{frequenza di sparo} del neurone viene modellata con una funzione di attivazione $f$       
    \end{itemize}
\end{frame}


\begin{frame}{Reti Neurali}
    \framesubtitle{Funzioni di Attivazione}
    \begin{block}{Definizione} 
        \large Una \emph{funzione di attivazione} è una funzione matematica non lineare usata per modellare l'output di un neurone. L'input è dato dalla somma pesata dei segnali in ingresso al neurone
    \end{block}\pause
    
    \bigskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{1.5em}
        \item \emph{\large Rectifier Linear Unit}
        \item \emph{\large Sigmoide}
        \item \emph{\large Tangente Iperbolica}
        \item \emph{\large Softplus}
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Rectifier Linear Unit}
    \begin{block}{Definizione} 
        \large La \emph{Rectifier Linear Unit (ReLU)} $r: \mathbb{R} \rightarrow [0, +\infty)$ è definita come $r(x) = \max(0,x)$  
    \end{block}\pause
    
    \bigskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Si differenzia da una funzione di tipo lineare per metà del suo dominio in quanto $\forall x < 0, max(0,x) = 0$
        \item \large Presenta un punto di discontinuità in $x = 0$
        \item \large La sua derivata è pari a $r^{\prime}(x) = \mathds{1}(x \geq 0)$
    \end{itemize}
\end{frame}


\begin{frame}{Reti Neurali}
    \framesubtitle{Rectifier Linear Unit}
    
    \begin{center}
      \includegraphics[scale = 0.6]{relu.jpeg}
    \end{center}
  
    \bigskip 
  
  \begin{itemize}
    \setlength\itemsep{1em}
    \item[] \large \emph{Rappresentazione grafica ReLU}
  \end{itemize}       
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Sigmoide}
    \begin{block}{Definizione} 
        \large La \emph{Sigmoide} $\sigma: \mathbb{R} \rightarrow [0, 1]$ è definita come $\sigma(x) = \frac{1}{(1 + e^{-x})}$ 
    \end{block}\pause
    
    \bigskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Per elevati valori negativi di input la sigmoide restituisce 0: il neurone non spara affatto
        \item \large Per elevati valori positivi la sigmoide restituisce 1: il neurone satura e spara con frequenza di sparo pari a 1
        \item \large La sua derivata è uguale a $\sigma^{\prime}(x) = \sigma(x)(1 - \sigma(x))$
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Sigmoide}
    
    \begin{center}
      \includegraphics[scale = 0.6]{sigmoid.jpeg}
    \end{center}
  
    \bigskip 
  
  \begin{itemize}
    \setlength\itemsep{1em}
    \item[] \large \emph{Rappresentazione grafica Sigmoide}
  \end{itemize}       
\end{frame} 



\begin{frame}{Reti Neurali}
    \framesubtitle{Tangente Iperbolica}
    \begin{block}{Definizione} 
        \large La \emph{Tangente Iperbolica} $\tanh: \mathbb{R} \rightarrow [-1, 1]$ è definita come $\tanh(x) = 2\sigma(2x) - 1$ 
    \end{block}\pause
    
    \bigskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large La tangente iperbolica è una sigmoide scalata
        \item \large A differenza della sigmoide passa dall'origine per $x = 0$ 
        \item \large La sua derivata è uguale a $\tanh^{\prime}(x) = 1 - \tanh^{2}(x)$
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Tangente Iperbolica}
    
    \begin{center}
      \includegraphics[scale = 0.6]{tanh.jpeg}
    \end{center}
  
    \bigskip 
  
  \begin{itemize}
    \setlength\itemsep{1em}
    \item[] \large \emph{Rappresentazione grafica Tangente Iperbolica}
  \end{itemize}       
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Softplus}
    \begin{block}{Definizione} 
        \large La \emph{Softplus} $s: \mathbb{R} \rightarrow (0, +\infty)$ è definita come $s(x) = \log(1 + e^x)$ 
    \end{block}\pause
    
    \bigskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large La softplus è una buona approssimazione della ReLU
        \item \large Viene solitamente usata per sostituire la ReLU perché non presenta punti di discontinuità
        \item \large La sua derivata è uguale a $s^{\prime}(x) = \frac{1}{(1 + e^{-x})} = \sigma(x)$
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Softplus}
    
    \begin{center}
      \includegraphics[scale = 0.45]{softplus_vs_rectifier.png}
    \end{center}
  
    \smallskip
  
  \begin{itemize}
    \setlength\itemsep{1em}
    \item[] \large \emph{Confronto grafico tra ReLU e Softplus}
  \end{itemize}       
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Rete Neurale}
    
    \begin{block}{Definizione} 
        \large Una \emph{Rete Neurale} è composta da un certo numero di neuroni organizzati in insiemi distinti chiamati \emph{livelli} o \emph{layer}
    \end{block}\pause
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large I livelli sono connessi tra loro e sono posizionati uno di seguito all'altro in modo da formare una sequenza
        \item \large I livelli intermedi prendono il nome di \emph{hidden}
        \item \large L'output dei neuroni di un livello diventano l'input dei neuroni del livello successivo        
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Rete Neurale}
    
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large Quando si effettua il conteggio dei livelli di una rete non si considera il livello di input
        \item \large Una rete a \emph{singolo livello} non presenta livelli hidden    
        \item \large Per determinare la grandezza di una rete ci si concentra sul numero di neuroni e sui relativi pesi ad essi associati
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Livello Fully-Connected}
    
    \begin{block}{Definizione} 
        \large Un livello è di tipo \emph{Fully-Connected} quando i neuroni che lo compongono sono completamente connessi ai neuroni del livello successivo e non sono collegati tra loro internamente 
    \end{block}\pause
    
    \begin{itemize} [<+->]
        \setlength\itemsep{1.5em}
        \item \large I pesi dei neuroni di ciascun livello sono salvati all'interno di matrici
        \item \large Le righe di una matrice identificano i neuroni del livello mentre le colonne contengono i pesi di ciascun neurone
        \item \large La struttura a livelli di una rete neurale permette di sfruttare le potenzialità del calcolo matriciale 
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Livello Fully-Connected}
    
    \begin{center}
      \includegraphics[scale = 0.35]{fully_connected.jpeg}
    \end{center}
  
    \bigskip
  
    \begin{itemize}
        \setlength\itemsep{1em}
        \item[] \large \emph{Una rete neurale a 3 livelli}
    \end{itemize}       
\end{frame} 



\begin{frame}{Reti Neurali}
    \framesubtitle{Funzionamento}
    
    Il processo di apprendimento di una rete neurale è suddiviso in quattro fasi distinte \pause
    
    \bigskip
    \bigskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \emph{\large Inizializzazione dei pesi}
        \item \emph{\large Forward Propagation}
        \item \emph{\large Calcolo della Funzione di Perdita}
        \item \emph{\large Back Propagation}
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Inizializzazione dei pesi}
    
    \smallskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large Al momento della nascita gli esseri umani non sono in grado di discriminare nessun tipo di oggetto a causa del mancato addestramento della loro rete neurale biologica
        \item \large Per riprodurre questo comportamento, all'inizio della fase di training, i pesi sinaptici $w_i$ di ciascun livello vengono inizializzati in maniera casuale     
    \end{itemize}
\end{frame}



\begin{frame}{Reti Neurali}
    \framesubtitle{Forward Propagation}
    
    \begin{block}{Definizione} 
        \large La \emph{Forward Propagation} è il meccanismo utilizzato da una rete neurale per associare ad un'immagine una determinata classe
    \end{block}\pause
    
    \smallskip
        
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
        \item \large L'output dei neuroni del livello $i$ viene moltiplicato per la matrice dei pesi del livello $i+1$ ottenendo il vettore $v$
        \item \large Al vettore $v$ viene aggiunto il vettore dei bias del livello $i+1$
        \item \large L'output del livello $i+1$ si ottiene applicando la funzione di attivazione $f$ ad ogni entry del vettore $v$
        \item \large Le operazioni precedenti sono svolte per tutti i livelli ad eccezione dell'ultimo
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Calcolo della funzione di perdita}
    
    \begin{block}{Definizione} 
        \large Una \emph{funzione di perdita} $L$ viene utilizzata per determinare l'errore di classificazione di una rete neurale
    \end{block}\pause
        
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
        \item \large La funzione di perdita più usata è la \emph{Mean Squared Error (MSE)} $L = \frac{1}{2} \sum (y - o)^{2}$
        \item \large $y$ identifica l'etichetta dell'esempio considerato mentre $o$ l'output della rete 
        \item \large Minimizzando la funzione di perdita $L$ si riduce l'errore di una rete neurale
        \item \large Calcolando la derivata di $L$ in funzione dei pesi $w_i$ si cerca di individuare il minimo globale della funzione di perdita
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Funzione di perdita}
    
    \begin{center}
      \includegraphics[scale = 0.7]{Loss.png}
    \end{center}
  
    \smallskip
  
    \begin{itemize}
        \setlength\itemsep{1em}
        \item[] \large \emph{Mean Squared Error (MSE). I pesi $w_1$ e $w_2$ sono le variabili indipendenti. La funzione di perdita $L$ è la variabile dipendente}
    \end{itemize}       
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Back Propagation}
    
    \begin{block}{Definizione} 
        \large La \emph{Back Propagation} è il meccanismo utilizzato da una rete neurale per correggere gli errori di classificazione. Vengono individuati i pesi $w_i$ che hanno influenzato maggiormente l'errore commesso e viene aggiornato il loro valore in modo da ridurre la funzione di perdita
    \end{block}\pause
    
    \bigskip 
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Per calcolare la derivata della funzione $L$ in funzione dei pesi $w_i$ viene usata la \emph{regola della catena (chain rule)}
        \item \large Questa regola è usata per trovare la derivata di una funzione composta
    \end{itemize}      
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Aggiornamento dei Pesi e Learning Rate}
    
    \begin{itemize} [<+->]
        \setlength\itemsep{1.5em}
        \item \large Il nuovo valore del peso $w_i$ è dato dalla regola di aggiornamento  $w_i = w_i - \eta\frac{\partial L}{\partial w_i} = w_i + \Delta w_i$ con $\eta > 0$
        \item \large Il \emph{learning rate} $\eta$ è un parametro usato per controllare la velocità di aggiornamento dei pesi 
        \item \large Un learning rate alto comporta aggiornamenti rapidi, un tempo di esecuzione più basso, ma una maggiore probabilità di finire in un minimo locale
        \item \large Un learning rate basso diminuisce la probabilità di finire in un minimo locale, ma allunga notevolmente i tempi di esecuzione
    \end{itemize}      
\end{frame}

\begin{comment}
\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Learning Rate}
    
    \begin{center}
      \includegraphics[scale = 0.6]{HighLR.png}
    \end{center}
    
    \bigskip
    \smallskip
    
    \begin{itemize}
        \setlength\itemsep{1em}
        \item[] \large \emph{Un learning rate $\eta$ elevato comporta ampi salti e finisce rapidamente in un minimo locale}
    \end{itemize}    
\end{frame} 

\end{comment}

\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    
    \begin{center}
      \includegraphics[scale = 0.35]{back1.png}
    \end{center}
    
    \vspace{-1em}
    
    \begin{columns}
        \begin{column}{0.48\textwidth}
            \begin{equation*}
                 \begin{split}
                    & z_j^{h} = \sum_{i=0}^{n} w_{ij}^{h}x_i \\[5pt]
                    & h_j = f(z_j^{h})
                \end{split}
            \end{equation*}            
        \end{column}
        \begin{column}{0.48\textwidth}
            \begin{equation*}
                \begin{split}
                    & z^{o} = \sum_{j=0}^{m} w_{j}^{o}h_j \\[5pt]
                    & o = f(z^{o})
                \end{split}
            \end{equation*}
        \end{column}
    \end{columns}    
    
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Derivata di $L$ in funzione del peso $w_j^{o}$ 
        
        \bigskip
        \begin{center}
         \large $ \frac{\partial L}{\partial w_j^{o}} =  \frac{\partial L}{\partial o} \cdot \frac{\partial o}{\partial z^{o}} \cdot \frac{\partial z^{o}}{\partial w_j}$         
         \end{center}
         
         \bigskip
         \setbeamertemplate{itemize items}[square]
         \begin{itemize}
            \setlength\itemsep{2.5em}
            \item \large $\frac{\partial L}{\partial o} = \frac{\partial}{\partial o} \big[ \frac{1}{2}(y - o)^{2} \big] = -(y - o)$
            \item \large $\frac{\partial o}{\partial z^{o}} = f^{\prime}(z^{o})$
            \item \large $\frac{\partial z^{o}}{\partial w_j^{o}} = h_j$ 
         \end{itemize}
    \end{itemize}
\end{frame}


\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large Risultato della derivata di $L$ in funzione del peso $w_j^{o}$
        \bigskip
        \begin{center}
         \large $ \frac{\partial L}{\partial w_j^{o}} =  -(y - o) \cdot f^{\prime}(z^{o}) \cdot h_j = -\delta_j^{o}h_j$         
         \end{center}
         \item \large Aggiornamento del peso $w_j^{o}$
         \bigskip
         \begin{center}
         \large $ \Delta w_j^{o} = \eta\delta_j^{o}h_j$         
         \end{center}       
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    
    \begin{center}
      \includegraphics[scale = 0.4]{back2.png}
    \end{center}
    
    \bigskip  

     \begin{equation*}
         \frac{\partial L}{\partial w_{ij}^{h}} =  \frac{\partial L}{\partial o} \cdot \frac{\partial o}{\partial z^{o}} \cdot \frac{\partial z^{o}}{\partial h_j} \cdot \frac{\partial h_j}{\partial z_j^{h}} \cdot \frac{\partial z_j^{h}}{\partial w_{ij}^{h}}
     \end{equation*}            
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        
        \item[] \large \begin{center} $\frac{\partial L}{\partial o} = \frac{\partial}{\partial o} \big[ \frac{1}{2}(y - o)^{2} \big] = -(y - o)$ \end{center}
        \item[] \large \begin{center} $\frac{\partial o}{\partial z^{o}} = f^{\prime}(z^{o})$ \end{center}
        \item[] \large \begin{center} $\frac{\partial z^{o}}{\partial h_j} = w_j^{o}$ \end{center}
        \item[] \large \begin{center} $\frac{\partial h_j}{\partial z_j^{h}} = f^{\prime}(z_j^{h})$ \end{center}
        \item[] \large \begin{center} $\frac{\partial z_j^{h}}{\partial w_{ij}^{h}} = x_i$ \end{center}          
    \end{itemize}
\end{frame}


\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large Risultato della derivata di $L$ in funzione del peso $w_{ij}^{h}$
        \bigskip 
        \begin{center}
         \large $ \frac{\partial L}{\partial w_{ij}^{h}} =  -(y - o) \cdot f^{\prime}(z^{o}) \cdot w_j^{o} \cdot f^{\prime}(z_j^{h}) \cdot x_i = -\delta_j^{h}x_i$         
         \end{center}
         \item \large Aggiornamento del peso $w_{ij}^{h}$
         \bigskip
         \begin{center}
         \large $ \Delta w_{ij}^{h} = \eta\delta_j^{h}x_i$         
         \end{center}       
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Rete Neurale Convoluzionale}
    
    \begin{block}{Definizione}  
    Una \emph{Rete Neurale Convoluzionale} è una variante di una rete neurale classica. Permette la condivisione dei pesi sinaptici tra i neuroni di un livello e consente di discriminare le varie feature che compongono un'immagine
    \end{block}\pause
    
    \bigskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
        \item \large Viene definito un nuovo tipo di livello: il \emph{Livello Convoluzionale}
        \item \large Un livello convoluzionale è formato da diversi \emph{filtri}
        \item \large La \emph{profondità (depth)} di un livello convoluzionale è data dal numero di filtri che lo compongono
    \end{itemize}    
\end{frame}


\begin{frame}{Reti Neurali}
    \framesubtitle{Filtri e Livelli Convoluzionali}  
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
         \item \large I filtri sono le matrici contenenti i pesi sinaptici del livello convoluzionale
        \item \large Ogni filtro ricerca all'interno delle immagini della rete una o più \emph{feature}: linee, curve, pattern
        \item \large Per apprendere nel miglior modo possibile il contenuto semantico di un'immagine, la rete deve saper ricercare feature sempre più complesse
        \item \large Mettendo in sequenza più livelli convoluzionali si possono ottenere feature complesse
        \item \large L'output di un generico livello convoluzionale $i$ diventa l'input del successivo livello $i+1$. Le feature prodotte da $i$ sono meno complesse di quelle ottenute da $i+1$
    \end{itemize}   
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Funzionamento}
    
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large I pesi dei filtri di un livello convoluzionale sono inizializzati in maniera casuale
        \item \large Vengono utilizzate le stesse funzioni di attivazione e le stesse funzioni di perdita dei livelli fully-connected
        \item \large La forward e la back propagation sono le uniche fasi definite diversamente
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Forward Propagation} 
    
    \begin{center}
      \includegraphics[scale = 0.4]{fCNN1.png}
    \end{center}
    
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Forward Propagation} 
    
    \begin{center}
      \includegraphics[scale = 0.7]{forward_conv1.png}
    \end{center}
  
    \smallskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Le matrici di input e di output di un livello convoluzionale prendono il nome di \emph{feature map}
        \item \large I filtri sono meglio conosciuti con il nome di \emph{kernel}
    \end{itemize}
    
\end{frame}


\begin{frame}{Reti Neurali}
    \framesubtitle{Forward Propagation} 
    
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
        \item \large All'inizio della forward propagation, il kernel viene sovrapposto alla parte superiore sinistra della feature map di input
        \item \large Viene eseguita la \emph{convoluzione} tra le due sottomatrici ed il risultato ottenuto viene salvato nella feature map di output 
        \item \large Il kernel viene spostato di una posizione verso destra e viene rieseguita nuovamente la convoluzione  
        \item \large Terminata la riga, il kernel viene posizionato nuovamente nella parte sinistra della feature map di input, ma una riga più in basso
        \item \large Gli ultimi due passaggi vengono ripetuti fino a quando non è stata riempita completamente tutta la feature map di output
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Forward Propagation} 
    
    \bigskip
    
    \begin{center}
      \includegraphics[scale = 0.4]{forward_flipped.png}
    \end{center}
  
    \bigskip \smallskip
        
    \begin{itemize}
        \setlength\itemsep{1em}
        \item[] \large \emph{Il kernel viene ruotato di $180^\circ$ per poter eseguire la convoluzione}
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Forward Propagation} 
    
    \begin{center}
      \includegraphics[scale = 0.35]{forward_conv2.png}
    \end{center}
        
    \begin{itemize}
        \setlength\itemsep{1em}
        \item[] \large \emph{Forward Propagation di un livello convoluzionale}
    \end{itemize}
\end{frame}


\begin{comment}

\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Forward Propagation} 
    
    \begin{center}
      \includegraphics[scale = 0.7]{forward_conv.png}
    \end{center}
  
    \smallskip
    
    \begin{itemize}
        \setlength\itemsep{1em}
        \item[] \large \emph{Esempio di Forward Propagation. La matrice di input è un $7 \times 7$. Il filtro è un $3 \times 3$ e l'output è un $5 \times 5$}
    \end{itemize}
\end{frame}

\end{comment}

\begin{frame}{Reti Neurali}
    \framesubtitle{Considerazioni Forward Propagation} 
    
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large Al termine della forward propagation, la funzione di attivazione $f$ viene applicata ad ogni elemento della feature map di output
        \item \large Un kernel è una matrice quadrata di dimensione $K \times K$
        \item \large La feature map di input ha dimensione $W \times H$ con $W = H$ 
        \item \large La feature map di output è una matrice quadrata di dimensione $O \times O$ con $O = (W - K) + 1$
        %\item \large La matrice di output risultante è più piccola rispetto a quella di input
        %\item \large Una sequenza di diversi livelli convoluzionali comporta la perdita di una grande quantità di informazione spaziale
        %\item \large Il \emph{padding} è il meccanismo per mezzo del quale è possibile avere una matrice di output della stessa dimensione di quella di input 
    \end{itemize}
\end{frame}

\begin{comment}
\begin{frame}{Reti Neurali}
    \framesubtitle{Padding} 
    
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
        \item \large Il padding si ottiene aggiungendo un certo numero di zeri in cima ed in fondo alle righe e alle colonne della matrice di input
        \item \large L'operazione viene fatta prima che effettuata la convoluzione con i vari filtri del livello
        \item \large Il numero di zeri da aggiungere è dato da $P = \frac{(K - 1)}{2}$
        \item \large La dimensione della matrice di output è uguale a
         \bigskip            
         \begin{center} 
             $O = (W - K + 2P) + 1$ \\
             \bigskip
             $O = (W - K + 2 \cdot \frac{(K - 1)}{2}) + 1$ \\
             \bigskip
             $O = W$ 
         \end{center}
                     
    \end{itemize}
\end{frame}
\end{comment}

\begin{frame}{Reti Neurali}
    \framesubtitle{Back Propagation}
    
    \begin{block}{Definizione}
    La Back Propagation di una rete neurale convoluzionale ha come obiettivo l'aggiornamento dei pesi contenuti nei kernel di un livello. Per ciascuno dei pesi di un kernel viene calcolata la derivata parziale $\frac{\partial L}{\partial w_{m^{\prime}, n^{\prime}}^l}$ che rappresenta l'influenza del peso $w_{m^{\prime}, n^{\prime}}^l$ sulla funzione di perdita $L$
    \end{block}\pause
    
    \bigskip
    
     \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large La Back Propagation viene suddivisa in due fasi distinte
        
        \bigskip
        
        \setbeamertemplate{itemize items}[square] 
        \begin{itemize} [<+->] 
        \setlength\itemsep{1.5em}
            \item \large Il calcolo della matrice degli errori $\delta$
            \item \large L'aggiornamento dei pesi del kernel                     
        \end{itemize}
    \end{itemize}     
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Calcolo matrice dei $\delta$} 
    
    \begin{center}
      \includegraphics[scale = 0.4]{back_conv1.png}
    \end{center}
  
    \smallskip
    
    \begin{itemize}
        \setlength\itemsep{1em}
        \item[] \large \emph{Le linee tratteggiate presenti nella feature map di output individuano la regione dei pixel influenzati dal pixel $x_{i^{\prime},j^{\prime}}$. \\ $k_1$ e $k_2$ definiscono la grandezza della regione considerata}
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Calcolo matrice dei $\delta$}
    
     \begin{itemize} [<+->]
         \setlength\itemsep{2em}
         \item \large L'influenza del pixel $x_{i^{\prime},j^{\prime}}$ sulla funzione di perdita $L$ è data da
         
         \vspace{-0.7em}
         
         \begin{align*}
         \hspace{-1cm}         
               \delta_{i^{\prime},j^{\prime}}^l = \frac{\partial L}{\partial x_{i^{\prime},j^{\prime}}^l}                 
         \end{align*}         
         
         \item \large Applicando la regola della catena si ottiene
     
    \begin{align*}        
           \frac{\partial L}{\partial x_{i^{\prime},j^{\prime}}^{l}} &= \sum_{m = 0}^{k_1 -1} \sum_{n = 0}^{k_2 -1} \frac{\partial L}{\partial x_{i^{\prime}-m, j^{\prime}-n}^{l+1}}\frac{\partial x_{i^{\prime}-m, j^{\prime}-n}^{l+1}}{\partial x_{i^{\prime},j^{\prime}}^l} \\[10pt]            
              &= \sum_{m = 0}^{k_1 -1} \sum_{n = 0}^{k_2 -1} \delta^{l+1}_{i^{\prime}-m, j^{\prime}-n} \frac{\partial x_{i^{\prime}-m, j^{\prime}-n}^{l+1}}{\partial x_{i^{\prime},j^{\prime}}^l}       
     \end{align*}    
    \end{itemize}     
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Calcolo matrice dei $\delta$}
   
     
    \begin{align*}
        \hspace{2em}          
          \frac{\partial L}{\partial x_{i^{\prime},j^{\prime}}^{l}} &= \sum_{m = 0}^{k_1 - 1} \sum_{n = 0}^{k_2 - 1} \delta^{l+1}_{i^{\prime} - m,j^{\prime} - n} w_{m,n}^{l+1} f'\left(x_{i',j'}^{l}\right) \\[10pt]
& = \text{rot}_{180^\circ} \left\{ \sum_{m = 0}^{k_1 - 1} \sum_{n = 0}^{k_2 - 1} \delta^{l+1}_{i^{\prime} + m,j^{\prime} + n} w_{m,n}^{l+1} \right\} f'\left(x_{i',j'}^{l}\right) \\[10pt]
&= \delta^{l+1}_{i',j'} \ast \text{rot}_{180^\circ} \left\{ w_{m,n}^{l+1} \right\} f'\left(x_{i',j'}^{l} \right)  
     \end{align*}        
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Aggiornamento dei pesi} 
    
    \begin{center}
      \includegraphics[scale = 0.4]{back_conv.png}
    \end{center}
  
    \smallskip
    
    \begin{itemize}
        \setlength\itemsep{1em}
        \item[] \large \emph{Durante la fase di forward propagation, il peso $w_{m^{\prime}, n^{\prime}}$ ha contribuito a calcolare tutti i valori che costituiscono la feature map di output}
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Aggiornamento dei pesi}
     \smallskip
     \begin{itemize} [<+->]
     \setlength\itemsep{1em}
     \item \large Il calcolo di $\frac{\partial L}{\partial w_{m^{\prime}, n^{\prime}}^l}$ usando la regola della catena è dato da
     
    \begin{equation*}
        \begin{split}
            \frac{\partial L}{\partial w_{m^{\prime}, n^{\prime}}^l} & = \sum_{i=0}^{W-K} \sum_{j=0}^{W-K} \frac{\partial L}{\partial x_{i,j}^{l}} \frac{\partial x_{i,j}^{l}}{\partial w_{m^{\prime},n^{\prime}}^l} \\[10pt]
           & = \sum_{i = 0}^{W - K}\sum_{j = 0}^{W - K} \delta_{i,j}^l \cdot o_{i + m^{\prime},j + n^{\prime}}^{l-1} \\[10pt]            
           & = rot_{180^\circ} \{ \delta_{i,j}^l \} * o_{m^{\prime},n^{\prime}}^{l-1} 
        \end{split}          
    \end{equation*} 
    
    \bigskip
    
    \item \large $x_{i,j}^l = \sum_{m} \sum_{n} w_{m,n}^{l}o_{i+m, j+n}^{l-1} + b^l$         
    \item \large $o_{m^{\prime},n^{\prime}}^{l-1} = f(x_{i^{\prime},j^{\prime}}^{l-1})$    
    \end{itemize}     
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Aggiornamento dei pesi}
    
    \bigskip
    
     \begin{itemize} [<+->]
     \setlength\itemsep{3em}
    \item \large Il risultato della convoluzione tra $\delta_{i,j}^l$ e $o_{m^{\prime},n^{\prime}}^{l-1}$ individua il nuovo valore del peso $w_{m^{\prime}, n^{\prime}}^l$
    \item \large La convoluzione è svolta per ciascuno dei pesi che costituiscono un kernel     
    \end{itemize}     
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Back Propagation} 
    
    \bigskip
    
    \begin{center}
      \includegraphics[scale = 0.6]{back_flipped.png}
    \end{center}
  
    \bigskip \smallskip
        
    \begin{itemize}
        \setlength\itemsep{1em}
        \item[] \large \emph{La matrice degli errori $\delta$ deve essere ruotata di $180^\circ$ per poter eseguire la convoluzione}
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation} 
    
    \begin{center}
      \includegraphics[scale = 0.4]{back_conv4.png}
    \end{center}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation} 
    
    \begin{center}
      \includegraphics[scale = 0.4]{back_conv3.png}
    \end{center}
        
    \begin{itemize}
        \setlength\itemsep{1em}
        \item[] \large \emph{Il kernel aggiornato viene ricavato dalla convoluzione tra la matrice degli errori $\delta$ e la feature map di input}
    \end{itemize}
\end{frame} 

% /////////////////////////// Implementazione della Rete ///////////////////////////

\begin{frame}[c]
  \centering
  \bigskip \bigskip    
  \Huge Implementazione della Rete
\end{frame}

\begin{frame}{Implementazione della Rete}
    \framesubtitle{Obiettivo}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Sfruttando l'architettura per il calcolo parallelo \emph{CUDA}, si vuole costruire una rete neurale convoluzionale che permetta di riconoscere cifre numeriche scritte a mano
        \item \large Le cifre da riconoscere sono salvate come immagini in scala di grigio a 8 bit. Un pixel può assumere solo valori compresi nell'intervallo $[0,255]$
        \item \large L'output della rete è dato dalle 10 cifre numeriche che si vogliono riconoscere
        \item \large La rete riceve in input un'immagine e le associa la cifra numerica corrispondente
    \end{itemize}
\end{frame}


\begin{frame}{Implementazione della Rete}
    \framesubtitle{Dati}
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large La dimensione delle immagini che costituiscono gli esempi del training e del test set hanno una dimensione di $28 \times 28$
        \item \large Le etichette sono rappresentate da numeri interi positivi
        \item \large Il training ed il test set provengono dal database \emph{MNIST} e contengono rispettativamente $60000$ esempi di train e $10000$ di test 
    \end{itemize}
\end{frame}

\begin{frame}{Implementazione della Rete}
    \framesubtitle{Rete CUDA}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Per potere sfruttare le potenzialità del paradigma ad oggetti, il codice host della rete CUDA è scritto in C++
        \item \large I livelli implementati devono rispettare un'interfaccia comune e vengono modellati come classi
        \item \large Le funzioni abibite alla fase di train, di test e al calcolo dell'accuratezza sono contenute in unica classe chiamata \emph{Network}
    \end{itemize}
\end{frame}

\begin{frame}{Implementazione della Rete}
    \framesubtitle{Rete CUDA}
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
         \item \large Quando viene definito un livello, l'utente non deve inserire anche la dimensione di output del livello precedente, ma solo quella del livello che sta definendo
         \item \large I pesi iniziali della rete possono essere generati in maniera casuale
         \item \large Per migliorare l'accuratezza della rete, viene utilizzato il meccanismo delle \emph{epoche} che consiste nel ripetere più volte la forward e la back propagation sulla stessa immagine
        \item \large Le funzioni di attivazione implementate dalla rete sono la \emph{Sigmoide}, la \emph{Tangente Iperbolica} e la \emph{SoftPlus} 
    \end{itemize}
\end{frame}

\begin{frame}{Implementazione della Rete}
    \framesubtitle{Caratteristiche tecniche}
      \begin{itemize} [<+->] 
        \setlength\itemsep{1em}           
            \item \large Per implementare i livelli della rete sono stati usati gli \emph{unique pointer} perché permettono all'utente di non deve specificare il numero di livelli che vuole usare e non deve preoccuparsi di deallocarli al termine dell'esecuzione           
            \item \large All'inizio della fase di train, l'intero training set viene caricato nella GRAM utilizzando il meccanismo della \emph{pinned memory}
             \item \large Prima della fase di test, il training set viene deallocato e al suo posto viene caricato il test set sempre con il meccanismo \emph{pinned memory}
            \item \large Le strutture dati usate dai singoli livelli vengono deallocate automaticamente al termine dell'esecuzione del processo
        \end{itemize}
\end{frame}

\begin{frame}{Implementazione della Rete}
    \framesubtitle{Librerie}
      \begin{itemize} [<+->] 
        \setlength\itemsep{1em}           
            \item \large Per eseguire le varie operazioni algebriche è stata utilizzata la libreria CUDA chiamata \emph{cuBLAS}
            \item \large Durante la costruzione della rete, più di preciso quando si eseguivano i test intermedi, si è osservato che la funzione adibita al calcolo del prodotto tra matrici \emph{cublasGemm} risulta più lenta di \emph{cublasGemv} che effettua il prodotto tra matrici e vettori
            \item \large A fronte di questi risultati, si è scelto di usare la funzione \emph{cublasGemv} per eseguire i prodotti matrice-vettore
            \item \large I pesi iniziali della rete vengono generati in maniera casuale utilizzando l'algoritmo \emph{xorWow} contenuto nella libreria di CUDA \emph{cuRand}
        \end{itemize}
\end{frame}

\begin{frame}{Implementazione della Rete}
    \framesubtitle{Stream}
      \begin{itemize} [<+->] 
        \setlength\itemsep{3em}           
            \item \large Se la funzione \emph{cublasGemm} viene utilizzata insieme agli \emph{streams}, il tempo di calcolo per eseguire i prodotti tra matrici dovrebbe ridursi notevolmente
            \item \large Nel caso della rete CUDA, eseguire una \emph{cublasGemm} per stream su matrici relativamente piccole, sia per livelli convoluzionali che fully-connected, non ha portato a nessun miglioramento del tempo di calcolo
            \item \large Il tempo impiegato con e senza stream è lo stesso, questo è dovuto al fatto che l'overhead della \emph{cublasGemm} su diversi stream è maggiore dell'effettivo tempo di calcolo del prodotto matriciale
        \end{itemize}
\end{frame}


\begin{frame}{Implementazione della Rete}
    \framesubtitle{Rete Sequenziale}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Le accuratezze e i tempi di esecuzione della rete CUDA sono stati confrontati con quelli di una rete sequenziale chiamata \emph{EduCNN}
        \item \large La scelta della rete sequenziale è ricaduta sulla EduCNN perché consente di trovare buone accuratezze in un tempo adeguato
        \item \large Ammette sia livelli di tipo fully-connected che convoluzionali
        \item \large L'unica funzione di attivazione implementata è la sigmoide
        \item \large La EduCNN è scritta con il linguaggio di programmazione \emph{C++}
    \end{itemize}
\end{frame}


\begin{frame}{Implementazione della Rete}
    \framesubtitle{Differenze tra le due Reti}
    \smallskip
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Per ricavare i valori dei pesi iniziali, la EduCNN utilizza come generatore di numeri casuali l'algoritmo \emph{xorshift128}
        \item \large Quando si definisce un livello, la EduCNN richiede all'utente di specificare il numero di livelli che vuole usare e la dimensione di output del livello precedente
        \item \large La memoria allocata per ciascun livello deve essere eliminata dall'utente al termine dell'esecuzione del processo 
    \end{itemize}     
\end{frame}

\begin{frame}{Implementazione della Rete}
    \framesubtitle{Considerazioni}
    \begin{itemize} [<+->]
        \setlength\itemsep{1.5em}
        \item \large I calcoli interni alla rete vengono svolti usando il formato di dato \emph{double} per non perdere precisione numerica tra un passaggio e un altro
        \item \large All'inizio della fase di training i pixel delle immagini vengono riscalati nell'intervallo $[0,1]$ per poter essere compatibili con il formato di dato usato dalla rete 
    \end{itemize} 
    
\end{frame}

\begin{frame}{Implementazione della Rete}
    \framesubtitle{Configurazioni}
    \smallskip
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Il testing della rete viene effettuato combinando tra loro differenti tipi di livelli
        \item \large Le diverse combinazioni vengono chiamate \emph{Configurazioni}
        \item \large Vengono definite quattro configurazioni in modo da poter analizzare il comportamento della rete in determinate situazioni
    \end{itemize}     
\end{frame}

\begin{frame}{Implementazione della Rete}
    \framesubtitle{Configurazioni}
    \smallskip
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
        \item \large Il learning rate $\eta$ ed i pesi iniziali di ciascuna configurazione sono gli stessi per entrambe le reti 
        \item \large All'inizio della computazione, i pesi vengono posti ad un valore fisso di $0.001$ 
        \item \large Le configurazioni vengono inoltre testate generando i pesi iniziali in maniera casuale in modo da poter valutare le differenti accuratezze ottenute dalle due reti. Il seed scelto per effettuare i test è pari a $0$
        \item \large Il numero di nodi e di livelli per ciascuna configurazione viene scelto tenendo conto della componentistica hardware che si ha a disposizione per eseguire i test
    \end{itemize}     
\end{frame}

\begin{frame}{Implementazione della Rete}
    \framesubtitle{Configurazioni}   
    
    \begin{table}
        \centering
        \renewcommand\arraystretch{1.3}
        \begin{tabular}{| c | c | c |}
           \hline
           \emph{Livello} & \emph{Output} \\
           \hline
           Fully Connected & $300 \times 1$ \\
           \hline
           Fully Connected & $10 \times 1$ \\
           \hline
        \end{tabular}
    \caption{Configurazione 1}
    \end{table}
    
    \begin{table}
        \centering
        \renewcommand\arraystretch{1.3}
        \begin{tabular}{| c | c | c |}
           \hline
           \emph{Livello} & \emph{Output} & \emph{Dimensione Filtro} \\
           \hline
           Convoluzionale & $24 \times 24$ & $5 \times 5 \times 3$   \\  
           \hline
           Convoluzionale & $20 \times 20$ & $5 \times 5 \times 3$   \\ 
           \hline 
           Convoluzionale & $16 \times 16$ & $5 \times 5 \times 3$   \\ 
           \hline
           Fully Connected & $10 \times 1$ & \ding{55} \\
           \hline           
        \end{tabular}
    \caption{Configurazione 2}
    \end{table}
    
\end{frame}


\begin{frame}{Implementazione della Rete}
    \framesubtitle{Configurazioni}     
    
    \bigskip
    
    \begin{table}
        \centering
        \renewcommand\arraystretch{1.3}
        \begin{tabular}{| c | c | c |}
           \hline
           \emph{Livello} & \emph{Output} & \emph{Dimensione Filtro} \\
           \hline
           Convoluzionale & $24 \times 24$ & $5 \times 5 \times 3$   \\  
           \hline
           Fully Connected & $400 \times 1$ & \ding{55} \\
           \hline 
           Convoluzionale & $16 \times 16$ & $5 \times 5 \times 3$   \\ 
           \hline
           Fully Connected & $10 \times 1$ & \ding{55} \\
           \hline           
        \end{tabular}
    \caption{Configurazione 3}
    \end{table}   
    
\end{frame}


\begin{frame}{Implementazione della Rete}
    \framesubtitle{Configurazioni}
    
    \bigskip
    
     \begin{table}
        \centering
        \renewcommand\arraystretch{1.3}
        \begin{tabular}{| c | c | c |}
           \hline
           \emph{Livello} & \emph{Output} & \emph{Dimensione Filtro} \\
           \hline
           Convoluzionale & $24 \times 24$ & $5 \times 5 \times 3$   \\  
           \hline
           Fully Connected & $400 \times 1$ & \ding{55} \\
           \hline 
           Convoluzionale & $16 \times 16$ & $5 \times 5 \times 3$   \\ 
           \hline
           Fully Connected & $100 \times 1$ & \ding{55} \\
           \hline 
           Convoluzionale & $6 \times 6$ & $5 \times 5 \times 3$   \\ 
           \hline
           Fully Connected & $10 \times 1$ & \ding{55} \\
           \hline         
        \end{tabular}
    \caption{Configurazione 4}
    \end{table}    
\end{frame}

\begin{frame}{Implementazione della Rete}
    \framesubtitle{Configurazioni}

    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large La \emph{Configurazione 1} ha lo scopo di testare il comportamento della rete CUDA quando vengono utilizzati solo livelli di tipo fully-connected
        \item \large Allo stesso modo, la \emph{Configurazione 2} verifica come si comporta la rete quando viene utilizzata una sequenza di livelli convoluzionali    
        \item \large Le restanti configurazioni sono costruite alternado tra loro livelli convoluzionali di profondità tre e livelli fully-connected. Questo tipo di approccio serve a valutare se i valori di accuratezza ed i tempi di esecuzione ottenuti dalla rete CUDA sono validi anche per reti che ammettono una certa \emph{profondità}
    \end{itemize}     
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}

\begin{frame}{Implementazione della Rete}
    \framesubtitle{Cifar}

    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large Per poter analizzare più approfonditamente le potenzialità computazionali della rete CUDA, le \emph{Configurazioni 3} e \emph{4} vengono testate usando il training set ed il test set del database di immagini chiamato \emph{Cifar}
        \item \large In Cifar, le immagini che costituiscono gli esempi del training e del test set sono definite secondo il modello di colore \emph{RGB} e presentano tre differenti piani colore: rosso, verde, blu       
    \end{itemize}     
\end{frame}

\begin{frame}{Implementazione della Rete}
    \framesubtitle{Cifar}

    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large La dimensione di ciascuna immagine è $32 \times 32 \times 3$. Il valore della terza dimensione è pari al numero dei piani colore del modello RGB
        \item \large Le etichette sono rappresentate da numeri interi positivi
        \item \large Il training ed il test set contengono rispettativamente $50000$ esempi di train e $10000$ di test         
    \end{itemize}     
\end{frame}

\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[c]
  \centering
  \bigskip \bigskip    
  \Huge Analisi dei Risultati
\end{frame}

\begin{frame}{Analisi dei Risultati}
    \framesubtitle{Hardware}
    \smallskip
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Le varie configurazioni sono state eseguite su due differenti tipi di macchine
        \item \large La prima presenta le seguenti caratteristiche tecniche       
        \bigskip
        \setbeamertemplate{itemize items}[square] 
        \begin{itemize} [<+->] 
        \setlength\itemsep{2.5em}
            \item \large Processore Intel Core i7-4510 da 2.00GHz
            \item \large RAM da 6GB
            \item \large Scheda grafica Nvidia GeForce 820M da 1GB con Architettura Fermi
            \item \large Sistema operativo Ubuntu 17.10                    
        \end{itemize}        
    \end{itemize}     
\end{frame}

\begin{frame}{Analisi dei Risultati}
    \framesubtitle{Hardware}
    \smallskip
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Le caratteristiche tecniche della seconda macchina sono            
        \bigskip
        \setbeamertemplate{itemize items}[square] 
        \begin{itemize} [<+->] 
        \setlength\itemsep{2.5em}
            \item \large Processore Intel Core i7-6500u da 2.50GHz (3.10GHz in Turbo Boost)
            \item \large RAM da 12GB
            \item \large Scheda grafica Nvidia GeForce 940M da 4GB con Architettura Maxwell
            \item \large Sistema operativo Windows 10 Pro                    
        \end{itemize}        
    \end{itemize}     
\end{frame}

\begin{frame}{Analisi dei Risultati}
    \framesubtitle{Parametri}
    \smallskip
    \begin{itemize} [<+->]
        \setlength\itemsep{2.5em}
        \item \large I diversi valori del learning rate $\eta$ utilizzati per testare la rete sono stati ottenuti campionando l'intervallo $[0.001, 1.50]$ a step variabili
        \item \large Per ciascuna configurazione vengono ricavati i tempi di computazione della rete sequenziale, della rete CUDA ed il relativo speedup
        \item \large Le configurazioni mostrate nei risultati sono quelle che hanno ottenuto il massimo valore di accuratezza in entrambe le reti tra tutti i learning rate $\eta$ testati
    \end{itemize}     
\end{frame}


% ///////////////////////     PESI FISSATI A 0.01 /////////////////////////////////

\begin{frame}{Analisi dei Risultati}
    \framesubtitle{Risultati}

        \begin{table}
            \centering
            \renewcommand\arraystretch{1.3}
            \small
            \begin{tabular}{| c | c | c | c | c | c |}
                \hline
                \emph{Configurazione} & $\eta$ & \emph{Rete} & \emph{Accuratezza} & \emph{Tempo [h:m:s]} & \emph{Speedup} \\
                \hline
                \multirow{2}{*}{1} & \multirow{2}{*}{0.09} & EduCNN & 30.50\% & 00:03:17 & \multirow{2}{*}{3.12} \\ \cline{3-5} 
                                   &                       & CUDA   & 30.50\% & 00:01:03 & \\
                \hline
                \multirow{2}{*}{2} & \multirow{2}{*}{0.24} & EduCNN & 88.35\% & 00:03:04 & \multirow{2}{*}{2.60} \\ \cline{3-5} 
                                   &                       & CUDA   & 88.35\% & 00:01:11 & \\
                \hline
                \multirow{2}{*}{3} & \multirow{2}{*}{0.62} & EduCNN & 93.80\% & 00:09:59 & \multirow{2}{*}{3.10} \\ \cline{3-5} 
                                   &                       & CUDA   & 93.78\% & 00:03:13 & \\
                \hline
                \multirow{2}{*}{4} & \multirow{2}{*}{1.11} & EduCNN & 88.63\% & 00:11:20 & \multirow{2}{*}{3.16} \\ \cline{3-5} 
                                   &                       & CUDA   & 92.17\% & 00:03:35 & \\
                \hline
            \end{tabular}
            \caption{Risultati ottenuti eseguendo le configurazioni in ambiente Linux}          
        %\end{table} 
            %\caption            
    %{\tabular[t]{@{}l@{}}Risultati ottenuti dalle configurazioni scelte\endtabular}          
        \end{table}    
\end{frame}

\begin{frame}{Analisi dei Risultati}
    \framesubtitle{Risultati}

        \begin{table}
            \centering
            \renewcommand\arraystretch{1.3}
            \small
            \begin{tabular}{| c | c | c | c | c | c |}
                \hline
                \emph{Configurazione} & $\eta$ & \emph{Rete} & \emph{Accuratezza} & \emph{Tempo [h:m:s]} & \emph{Speedup} \\
                \hline
                \multirow{2}{*}{1} & \multirow{2}{*}{0.09} & EduCNN & 30.50\% & 00:04:58 & \multirow{2}{*}{4.51} \\ \cline{3-5} 
                                   &                       & CUDA   & 30.49\% & 00:01:06  & \\
                \hline
                \multirow{2}{*}{2} & \multirow{2}{*}{0.24} & EduCNN & 88.35\% & 00:05:05 & \multirow{2}{*}{3.91} \\ \cline{3-5} 
                                   &                       & CUDA   & 88.35\% & 00:01:18  & \\
                \hline
                \multirow{2}{*}{3} & \multirow{2}{*}{0.62} & EduCNN & 93.80\% & 00:15:31 & \multirow{2}{*}{5.03} \\ \cline{3-5} 
                                   &                       & CUDA   & 93.79\% & 00:03:05  & \\
                \hline
                \multirow{2}{*}{4} & \multirow{2}{*}{1.11} & EduCNN & 88.63\% & 00:16:31 & \multirow{2}{*}{4.67} \\ \cline{3-5} 
                                   &                       & CUDA   & 92.17\% & 00:03:32  & \\
                \hline
            \end{tabular}
            \caption{Risultati ottenuti eseguendo le configurazioni in ambiente Windows}          
        %\end{table} 
            %\caption            
    %{\tabular[t]{@{}l@{}}Risultati ottenuti dalle configurazioni scelte\endtabular}          
        \end{table}    
\end{frame}

\begin{frame}{Analisi dei Risultati}
    \framesubtitle{Analisi configurazioni con i pesi fissati}
    \smallskip
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
        \item \large Le prime tre configurazioni ottengono la stessa accuratezza in entrambe le reti e per ciascuno dei sistemi operativi, mentre la \emph{Configurazione 4} mostra una differenza del 6\%. 
        \item \large La \emph{Configurazione 4} è formata da tanti livelli con un elevato numero di nodi e questo comporta una maggiore perdita di precisione nel calcolo dei pesi rispetto alle altre configurazioni. L'accuratezza ottenuta dalla rete CUDA è migliore di quella della EduCNN grazie ad un parametro di compilazione chiamato \emph{fmad} impostato a false che privilegia la precisione alla velocità
        \item \large Gli speedup sono più alti su Windows perché l'esecuzione sequenziale necessita di più risorse da parte del sistema operativo rispetto a Linux
    \end{itemize}     
\end{frame}





% ///////////////////////   PESI CASUALI /////////////////////////////////


\begin{frame}{Analisi dei Risultati}
    \framesubtitle{Risultati}

         \begin{table}
            \centering
            \renewcommand\arraystretch{1.3}
            \small
            \begin{tabular}{| c | c | c | c | c | c |}
                \hline
                \emph{Configurazione} & $\eta$ & \emph{Rete} & \emph{Accuratezza} & \emph{Tempo [h:m:s]} & \emph{Speedup} \\
                \hline
                \multirow{2}{*}{1} & \multirow{2}{*}{0.09} & EduCNN & 92.18\% & 00:03:17 & \multirow{2}{*}{3.12} \\ \cline{3-5} 
                                   &                       & CUDA   & 92.51\% & 00:01:03  & \\
                \hline
                \multirow{2}{*}{2} & \multirow{2}{*}{0.24} & EduCNN & 86.07\% & 00:03:04 & \multirow{2}{*}{2.60} \\ \cline{3-5} 
                                   &                       & CUDA   & 80.57\% & 00:01:11 & \\
                \hline
                \multirow{2}{*}{3} & \multirow{2}{*}{0.62} & EduCNN & 94.32\% & 00:09:59 & \multirow{2}{*}{3.10} \\ \cline{3-5} 
                                   &                       & CUDA   & 93.11\% & 00:03:13 & \\
                \hline
                \multirow{2}{*}{4} & \multirow{2}{*}{1.11} & EduCNN & 9.58\% & 00:11:20 & \multirow{2}{*}{3.16} \\ \cline{3-5} 
                                   &                       & CUDA   & 9.58\% & 00:03:35 & \\
                \hline
            \end{tabular}
            \caption                        
    {\tabular[t]{@{}l@{}}Risultati ottenuti eseguendo le configurazioni a partire da \\ pesi iniziali generati casualmente in ambiente Linux \endtabular}          
        \end{table}    
\end{frame}

\begin{frame}{Analisi dei Risultati}
    \framesubtitle{Risultati}

           \begin{table}
            \centering
            \renewcommand\arraystretch{1.3}
            \small
            \begin{tabular}{| c | c | c | c | c | c |}
                \hline
                \emph{Configurazione} & $\eta$ & \emph{Rete} & \emph{Accuratezza} & \emph{Tempo [h:m:s]} & \emph{Speedup} \\
                \hline
                \multirow{2}{*}{1} & \multirow{2}{*}{0.09} & EduCNN & 92.36\% & 00:04:58 & \multirow{2}{*}{4.51} \\ \cline{3-5} 
                                   &                       & CUDA   & 92.51\% & 00:01:06 & \\
                \hline
                \multirow{2}{*}{2} & \multirow{2}{*}{0.24} & EduCNN & 87.35\% & 00:05:05 & \multirow{2}{*}{3.91} \\ \cline{3-5} 
                                   &                       & CUDA   & 80.57\% & 00:01:18  & \\
                \hline
                \multirow{2}{*}{3} & \multirow{2}{*}{0.62} & EduCNN & 93.78\% & 00:15:31 & \multirow{2}{*}{5.03} \\ \cline{3-5} 
                                   &                       & CUDA   & 93.11\% & 00:03:05 & \\
                \hline
                \multirow{2}{*}{4} & \multirow{2}{*}{1.11} & EduCNN & 9.58\% & 00:16:31 & \multirow{2}{*}{4.67} \\ \cline{3-5} 
                                   &                       & CUDA   & 9.58\% & 00:03:32  & \\
                \hline
            \end{tabular}
            \caption                        
    {\tabular[t]{@{}l@{}}Risultati ottenuti eseguendo le configurazioni a partire da \\ pesi iniziali generati casualmente in ambiente Windows \endtabular}          
        \end{table}    
\end{frame}

\begin{frame}{Analisi dei Risultati}
    \framesubtitle{Analisi configurazioni con i pesi iniziali casuali}
    \smallskip
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large L'uso dei numeri casuali migliora le accuratezze delle configurazioni \emph{1} e \emph{3} sia su Linux che su Windows. La rete CUDA ottiene un'accuratezza inferiore a quella della EduCNN nella \emph{Configurazione 2} ed entrambe le reti producono risultati pessimi nella \emph{Configurazione 4} a causa dell'overfitting
        \item \large Lo speedup ottenuto è lo stesso delle configurazioni a pesi fissi
    \end{itemize}     
\end{frame}


% /////////////////////// EPOCHE /////////////////////////////////

\begin{frame}{Analisi dei Risultati}
    \framesubtitle{Risultati}

        \begin{table}
            \centering
            \renewcommand\arraystretch{1.3}
            \small
            \begin{tabular}{| c | c | c | c | c | c |}
                \hline
                \emph{Configurazione} & $\eta$ & \emph{Rete} & \emph{Accuratezza} & \emph{Tempo [h:m:s]} & \emph{Speedup} \\
                \hline
                \multirow{2}{*}{1} & \multirow{2}{*}{0.09} & EduCNN & 31.96\% & 00:35:12 & \multirow{2}{*}{3.40} \\ \cline{3-5} 
                                   &                       & CUDA   & 31.96\% & 00:10:22 & \\
                \hline
                \multirow{2}{*}{2} & \multirow{2}{*}{0.24} & EduCNN & 90.11\% & 00:33:20 & \multirow{2}{*}{2.85} \\ \cline{3-5} 
                                   &                       & CUDA   & 90.11\% & 00:11:42 & \\
                \hline
                \multirow{2}{*}{3} & \multirow{2}{*}{0.62} & EduCNN & 97.49\% & 01:45:00 & \multirow{2}{*}{3.18} \\ \cline{3-5} 
                                   &                       & CUDA   & 97.70\% & 00:33:00 & \\
                \hline
                \multirow{2}{*}{4} & \multirow{2}{*}{1.11} & EduCNN & 13.68\% & 01:55:00 & \multirow{2}{*}{3.28} \\ \cline{3-5} 
                                   &                       & CUDA   & 18.87\% & 00:35:05 & \\
                \hline
            \end{tabular}
            \caption            
    {\tabular[t]{@{}l@{}}Risultati ottenuti eseguendo le configurazioni con \\ un numero di epoche pari a $10$ in ambiente Linux\endtabular}          
        \end{table}    
\end{frame}

\begin{frame}{Analisi dei Risultati}
    \framesubtitle{Risultati}

        \begin{table}
            \centering
            \renewcommand\arraystretch{1.3}
            \small
            \begin{tabular}{| c | c | c | c | c | c |}
                \hline
                \emph{Configurazione} & $\eta$ & \emph{Rete} & \emph{Accuratezza} & \emph{Tempo [h:m:s]} & \emph{Speedup} \\
                \hline
                \multirow{2}{*}{1} & \multirow{2}{*}{0.09} & EduCNN & 31.96\% & 00:45:46 & \multirow{2}{*}{4.38} \\ \cline{3-5} 
                                   &                       & CUDA   & 31.96\% & 00:10:27 & \\
                \hline
                \multirow{2}{*}{2} & \multirow{2}{*}{0.24} & EduCNN & 90.11\% & 00:47:13 & \multirow{2}{*}{3.67} \\ \cline{3-5} 
                                   &                       & CUDA   & 90.11\% & 00:12:51 & \\
                \hline
                \multirow{2}{*}{3} & \multirow{2}{*}{0.62} & EduCNN & 97.49\% & 02:32:02 & \multirow{2}{*}{4.90} \\ \cline{3-5} 
                                   &                       & CUDA   & 97.71\% & 00:31:00 & \\
                \hline
                \multirow{2}{*}{4} & \multirow{2}{*}{1.11} & EduCNN & 13.68\% & 02:43:01 & \multirow{2}{*}{4.69} \\ \cline{3-5} 
                                   &                       & CUDA   & 18.88\% & 00:34:44 & \\
                \hline
            \end{tabular}
            \caption            
    {\tabular[t]{@{}l@{}}Risultati ottenuti eseguendo le configurazioni con \\ un numero di epoche pari a $10$ in ambiente Windows\endtabular}          
        \end{table}    
\end{frame}

\begin{frame}{Analisi dei Risultati}
    \framesubtitle{Analisi configurazioni con epoche e pesi fissati}
    \smallskip
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Le configurazioni \emph{1}, \emph{2}, \emph{3} migliorano la loro accuratezza eseguendo l'intero processo di train più volte, mentre la \emph{Configurazione 4}, nonostante il valore di accuratezza della rete CUDA sia più preciso di quello della eduCNN, è affetta da overfitting
        \item \large Lo speedup su Windows rimane più alto di quello su Linux. La rete CUDA riduce il tempo per la fase di train da un massimo di circa 3 ore ad un massimo di circa 40 minuti nelle configurazioni più computazionalmente pesanti \emph{3} e \emph{4}
    \end{itemize}     
\end{frame}




% ////////////////////////// EPOCHE CASUALI /////////////////////////

\begin{frame}{Analisi dei Risultati}
    \framesubtitle{Risultati}

        \begin{table}
            \centering
            \renewcommand\arraystretch{1.3}
            \small
            \begin{tabular}{| c | c | c | c | c | c |}
                \hline
                \emph{Configurazione} & $\eta$ & \emph{Rete} & \emph{Accuratezza} & \emph{Tempo [h:m:s]} & \emph{Speedup} \\
                \hline
                \multirow{2}{*}{1} & \multirow{2}{*}{0.09} & EduCNN & 97.55\% & 00:35:12 & \multirow{2}{*}{3.40} \\ \cline{3-5} 
                                   &                       & CUDA   & 97.72\% & 00:10:22 & \\
                \hline
                \multirow{2}{*}{2} & \multirow{2}{*}{0.24} & EduCNN & 89.19\% & 00:33:20 & \multirow{2}{*}{2.85} \\ \cline{3-5} 
                                   &                       & CUDA   & 9.80\%  & 00:11:42 & \\
                \hline
                \multirow{2}{*}{3} & \multirow{2}{*}{0.62} & EduCNN & 97.65\% & 01:45:00 & \multirow{2}{*}{3.18} \\ \cline{3-5} 
                                   &                       & CUDA   & 97.78\% & 00:33:00 & \\
                \hline
                \multirow{2}{*}{4} & \multirow{2}{*}{1.11} & EduCNN & 9.58\% & 01:55:00 & \multirow{2}{*}{3.28} \\ \cline{3-5} 
                                   &                       & CUDA   & 9.58\% & 00:35:05 & \\
                \hline
            \end{tabular}
            \caption            
    {\tabular[t]{@{}l@{}}Risultati ottenuti eseguendo le configurazioni a partire da \\ pesi iniziali generati casualmente e numero di epoche \\ pari a $10$ in ambiente Linux \endtabular}                  
        \end{table}    
\end{frame}

\begin{frame}{Analisi dei Risultati}
    \framesubtitle{Risultati}

        \begin{table}
            \centering
            \renewcommand\arraystretch{1.3}
            \small
            \begin{tabular}{| c | c | c | c | c | c |}
                \hline
                \emph{Configurazione} & $\eta$ & \emph{Rete} & \emph{Accuratezza} & \emph{Tempo [h:m:s]} & \emph{Speedup} \\
                \hline
                \multirow{2}{*}{1} & \multirow{2}{*}{0.09} & EduCNN & 97.73\% & 00:45:46 & \multirow{2}{*}{4.38} \\ \cline{3-5} 
                                   &                       & CUDA   & 97.72\% & 00:10:27  & \\
                \hline
                \multirow{2}{*}{2} & \multirow{2}{*}{0.24} & EduCNN & 90.47\% & 00:47:13 & \multirow{2}{*}{3.67} \\ \cline{3-5} 
                                   &                       & CUDA   & 9.80\% & 00:12:51 & \\
                \hline
                \multirow{2}{*}{3} & \multirow{2}{*}{0.62} & EduCNN & 97.61\% & 02:32:02 & \multirow{2}{*}{4.90} \\ \cline{3-5} 
                                   &                       & CUDA   & 97.78\% & 00:31:00 & \\
                \hline
                \multirow{2}{*}{4} & \multirow{2}{*}{1.11} & EduCNN & 9.58\% & 02:43:01 & \multirow{2}{*}{4.69} \\ \cline{3-5} 
                                   &                       & CUDA   & 9.58\% & 00:34:44 & \\
                \hline
            \end{tabular}
            \caption            
    {\tabular[t]{@{}l@{}}Risultati ottenuti eseguendo le configurazioni a partire da \\ pesi iniziali generati casualmente e numero di epoche \\ pari a $10$ in ambiente Windows \endtabular}          
        \end{table}    
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}

\begin{frame}{Analisi dei Risultati}
    \framesubtitle{Risultati}

        \begin{table}
            \centering
            \renewcommand\arraystretch{1.3}
            \small
            \begin{tabular}{| c | c | c | c | c | c |}
                \hline
                \emph{Configurazione} & $\eta$ & \emph{Rete} & \emph{Accuratezza} & \emph{Tempo [h:m:s]} & \emph{Speedup} \\
                \hline
                \multirow{2}{*}{1} & \multirow{2}{*}{0.09} & EduCNN & 97.73\% & 00:45:46 & \multirow{2}{*}{4.38} \\ \cline{3-5} 
                                   &                       & CUDA   & 97.72\% & 00:10:27  & \\
                \hline
                \multirow{2}{*}{2} & \multirow{2}{*}{0.24} & EduCNN & 90.47\% & 00:47:13 & \multirow{2}{*}{3.67} \\ \cline{3-5} 
                                   &                       & CUDA   & 9.80\% & 00:12:51 & \\
                \hline
                \multirow{2}{*}{3} & \multirow{2}{*}{0.62} & EduCNN & 97.61\% & 02:32:02 & \multirow{2}{*}{4.90} \\ \cline{3-5} 
                                   &                       & CUDA   & 97.78\% & 00:31:00 & \\
                \hline
                \multirow{2}{*}{4} & \multirow{2}{*}{1.11} & EduCNN & 9.58\% & 02:43:01 & \multirow{2}{*}{4.69} \\ \cline{3-5} 
                                   &                       & CUDA   & 9.58\% & 00:34:44 & \\
                \hline
            \end{tabular}
            \caption            
    {\tabular[t]{@{}l@{}}Risultati ottenuti eseguendo le configurazioni \\ usando come database Cifar \endtabular}          
        \end{table}    
\end{frame}

\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Analisi dei Risultati}
    \framesubtitle{Analisi configurazioni con epoche e pesi iniziali casuali}
    \smallskip
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large L'utilizzo dei numeri casuali consente di ottenere un'accuratezza con un errore inferiore al 3\% per le configurazioni \emph{1} e \emph{3}. La \emph{Configurazione 4} rimane affetta da overfitting come per i pesi fissi.
        \item \large Lo speedup ottenuto è lo stesso delle configurazioni con le epoche e pesi fissi
    \end{itemize}     
\end{frame}

\begin{frame}[c]
  \centering
  \bigskip \bigskip    
  \Huge Conclusioni
\end{frame}

\begin{frame}{Conclusioni}
    \framesubtitle{Conclusioni}
    \smallskip
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large La rete CUDA consente di ottenere dei buoni risultati di accuratezza e dei bassi tempi di computazione rispetto alla corrispettiva rete sequenziale
        \item \large Utilizzando la rete CUDA si evita di sovraccaricare ulteriormente la CPU di lavoro e di riempire con grandi quantità di dati la memoria RAM 
        \item \large Non dipende da librerie di terze parti e può essere eseguita anche sul sistema operativo macOS
    \end{itemize}     
\end{frame}


\end{document}
