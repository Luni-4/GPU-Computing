\documentclass[
   %handout
 ]{beamer}
 
\usetheme{simple}
\usepackage{lmodern}
\usepackage[scale=2]{ccicons}

%Codifica dei font di input e di output
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% Converte file eps in pdf
\usepackage{epstopdf}

% Consente di rappresentare la funzione identità
\usepackage{dsfont}

% Consente di impostare le virgolette del discorso diretto
\usepackage{dirtytalk}

% Consente di impostare i commenti multiline
\usepackage{verbatim}

% Consente di disegnare grafi
\usepackage{tikz}

% Libreria per disegnare cerchi e frecce di tikz
\usetikzlibrary{arrows}

% Definizione della cartella contenente le immagini da usare
\graphicspath{{img/}}

% Disabilitare la trasparenza sulla pause
\setbeamercovered{invisible}

            
% TODO
% Finire Parte algoritmo di apprendimento
% Backpropagation
% Migliorare la spiegazione delle slide


% Watermark background (simple theme)
%\setwatermark{\includegraphics[height=8cm]{img/Heckert_GNU_white.png}}
     
%\institute{\url{http://github.com/famuvie}}

% Metadati di apertura del pdf
\hypersetup{
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdfencoding=auto
}

% Definizione degli autori
\author {
            \texorpdfstring{\hspace*{0.01em}{\Large Michele Valsesia}}{Michele Valsesia} 
            \texorpdfstring{\\ \bigskip}{e}
            \texorpdfstring{\hspace*{0.3em}{\Large Nicholas Aspes }}{Nicholas Aspes}
        }

\begin{document}

% Definizione del Titolo e Anno Accademico

\title{Implementazione di una \\ 
       Rete Convoluzionale in CUDA \bigskip}
        
\date{\Large Anno accademico 2018/2019}


% Titolo della Presentazione
     
\begin{frame}
\maketitle
\end{frame}

% //////////////////////////////// Introduzione ////////////////////////////////////////


\begin{frame}{Introduzione}
    \framesubtitle{Obiettivi}  
    
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large Descrivere brevemente l'architettura ed il funzionamento di una \emph{Rete Neurale}
        \item \large Motivare le differenti scelte implementative adottate durante lo svolgimento del progetto
        \item \large Valutare l'accuratezza e lo speed-up della rete rispetto ad una sua implementazione sequenziale       
    \end{itemize}  
\end{frame}


% //////////////////////////////// Parte Teorica Reti Neurali //////////////////////////

\begin{frame}[c]
  \centering
  \bigskip \bigskip    
  \Huge Reti Neurali
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Scopo}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Le \emph{Reti Neurali} vengono principalmente usate per la classificazione di immagini
       \item \large Il processo di classificazione consiste nell'assegnare ad un immagine un'etichetta che identifichi nel miglior modo possibile il suo contenuto semantico
       \item \large L'insieme delle immagini che hanno tutte la stessa etichetta costituiscono una \emph{classe}
       \item \large Le reti neurali ricevono in input un'immagine e forniscono in output la relativa classe 
    \end{itemize}
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Funzionamento}
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
        \item \large Una rete neurale deve \emph{apprendere} come assegnare correttamente le immagini alle varie classi
        \item \large Un \emph{esempio} è una coppia (immagine, etichetta)
        \item \large Un team di persone valuta il contenuto semantico di ciascuna immagine e assegna all'esempio l'etichetta corrispondente
        \item \large Il \emph{training set} ed il \emph{test set} sono insiemi di esempi
        \item \large Il training set viene usato per l'addestramento (training) della rete
        \item \large Il test set serve a controllare che la rete abbia imparato a discriminare correttamente le immagini
       
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Training}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Per ognuno degli esempi del training set
        
        \bigskip
        \bigskip
        
        \setbeamertemplate{itemize items}[square] 
        \begin{itemize} 
        \setlength\itemsep{3em}
            \item \large La rete riceve in input l'immagine relativa all'esempio considerato e l'associa ad una delle classi presenti
            \item \large Se la classe in output è diversa dall'etichetta dell'esempio, la rete corregge i suoi parametri interni e passa all'immagine successiva
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Testing}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large L'\emph{accuratezza} della rete è data dal rapporto tra il numero di esempi classificati scorrettamente ed il numero totale di esempi
        
        \item \large Per ognuno degli esempi del test set
        
        \bigskip
        
        \setbeamertemplate{itemize items}[square] 
        \begin{itemize} 
        \setlength\itemsep{2em}
            \item \large La rete riceve in input l'immagine dell'esempio considerato e l'associa ad una delle classi presenti
            \item \large Ogni volta che l'output della rete non corrisponde all'etichetta dell'esempio viene incrementato un contatore, necessario per il calcolo dell'accuratezza
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Significato Biologico}
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large Le \emph{Reti Neurali} nascono con lo scopo di modellare una rete neurale biologica
       \item \large Una rete neurale biologica si compone di unità cellulari di base: i \emph{neuroni}
       \item \large I neuroni sono collegati tra loro per mezzo di specifiche giunture chiamate \emph{sinapsi}
    \end{itemize}
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Neurone}
    
    \begin{center}
      \includegraphics[scale = 0.35]{neuron_model.jpeg}
    \end{center}
  
    \bigskip 
  
  \begin{itemize}
    \setlength\itemsep{1em}
    \item[] \large \emph{Modello matematico di un neurone}
  \end{itemize}       
\end{frame} 


\begin{frame}{Reti Neurali}
    \framesubtitle{Funzionamento Neurone}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Attraverso un meccanismo di eccitazione ed inibizione i pesi sinaptici controllano quanto un neurone sia influenzato dagli altri
       \item \large I segnali in ingresso al neurone vengono pesati dalle differenti sinapsi, trasportati dai dendriti all'interno del corpo cellulare e sommati tra loro
       \item \large Quando la somma supera una certa soglia, il neurone \emph{spara} un segnale lungo l'assone 
       \item \large La \emph{frequenza di sparo} del neurone viene modellata con una funzione di attivazione $f$       
    \end{itemize}
\end{frame}


\begin{frame}{Reti Neurali}
    \framesubtitle{Funzioni di Attivazione}
    \begin{block}{Definizione} 
        \large Una \emph{funzione di attivazione} è una funzione matematica non lineare che viene usata per calcolare l'output di un neurone. Il suo input è dato dalla somma pesata dei segnali in ingresso al neurone
    \end{block}\pause
    
    \bigskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{1.5em}
        \item \emph{\large Rectifier Linear Unit}
        \item \emph{\large Sigmoide}
        \item \emph{\large Tangente Iperbolica}
        \item \emph{\large Softplus}
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Rectifier Linear Unit}
    \begin{block}{Definizione} 
        \large La \emph{Rectifier Linear Unit (ReLU)} $r: \mathbb{R} \rightarrow [0, +\infty)$ è definita come $r(x) = \max(0,x)$  
    \end{block}\pause
    
    \bigskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Si differenzia da una funzione di tipo lineare per metà del suo dominio in quanto $\forall x < 0, max(0,x) = 0$
        \item \large Presenta un punto di discontinuità in $x = 0$
        \item \large La sua derivata è pari a $\mathds{1}(x \geq 0)$
    \end{itemize}
\end{frame}


\begin{frame}{Reti Neurali}
    \framesubtitle{Rectifier Linear Unit}
    
    \begin{center}
      \includegraphics[scale = 0.6]{relu.jpeg}
    \end{center}
  
    \bigskip 
  
  \begin{itemize}
    \setlength\itemsep{1em}
    \item[] \large \emph{Rappresentazione grafica ReLU}
  \end{itemize}       
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Sigmoide}
    \begin{block}{Definizione} 
        \large La \emph{Sigmoide} $\sigma: \mathbb{R} \rightarrow [0, 1]$ è definita come $\sigma(x) = \frac{1}{(1 + e^{-x})}$ 
    \end{block}\pause
    
    \bigskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Per elevati valori negativi di input la sigmoide restituisce 0: il neurone non spara affatto
        \item \large Per elevati valori positivi la sigmoide restituisce 1: il neurone satura e spara con frequenza di sparo pari a 1
        \item \large La sua derivata è uguale a $\sigma^{\prime}(x) = \sigma(x)(1 - \sigma(x))$
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Sigmoide}
    
    \begin{center}
      \includegraphics[scale = 0.6]{sigmoid.jpeg}
    \end{center}
  
    \bigskip 
  
  \begin{itemize}
    \setlength\itemsep{1em}
    \item[] \large \emph{Rappresentazione grafica Sigmoide}
  \end{itemize}       
\end{frame} 



\begin{frame}{Reti Neurali}
    \framesubtitle{Tangente Iperbolica}
    \begin{block}{Definizione} 
        \large La \emph{Tangente Iperbolica} $\tanh: \mathbb{R} \rightarrow [-1, 1]$ è definita come $\tanh(x) = 2\sigma(2x) - 1$ 
    \end{block}\pause
    
    \bigskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large La tangente iperbolica è una sigmoide scalata 
        \item \large La sua derivata è uguale a $\tanh^{\prime}(x) = 1 - \tanh^{2}(x)$
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Tangente Iperbolica}
    
    \begin{center}
      \includegraphics[scale = 0.6]{tanh.jpeg}
    \end{center}
  
    \bigskip 
  
  \begin{itemize}
    \setlength\itemsep{1em}
    \item[] \large \emph{Rappresentazione grafica Tangente Iperbolica}
  \end{itemize}       
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Softplus}
    \begin{block}{Definizione} 
        \large La \emph{Softplus} $s: \mathbb{R} \rightarrow (0, +\infty)$ è definita come $s(x) = \log(1 + e^x)$ 
    \end{block}\pause
    
    \bigskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large La softplus è una buona approssimazione della ReLU
        \item \large Viene solitamente usata per sostituire la ReLU perché non presenta punti di discontinuità
        \item \large La sua derivata è uguale a $s^{\prime}(x) = \frac{1}{(1 + e^{-x})}$
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Softplus}
    
    \begin{center}
      \includegraphics[scale = 0.45]{softplus_vs_rectifier.png}
    \end{center}
  
    \smallskip
  
  \begin{itemize}
    \setlength\itemsep{1em}
    \item[] \large \emph{Confronto grafico tra ReLU e Softplus}
  \end{itemize}       
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Rete Neurale}
    
    \begin{block}{Definizione} 
        \large Una \emph{Rete Neurale} è composta da un insieme di neuroni connessi tra loro in un grafo aciclico
    \end{block}\pause
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large  I neuroni sono organizzati in insiemi distinti chiamati \emph{livelli} o \emph{layer}
        \item \large I livelli sono posti uno di seguito all'altro in modo da formare una sequenza
        \item \large I livelli intermedi prendono il nome di \emph{hidden}
        \item \large L'output dei neuroni di un livello diventano l'input dei neuroni del livello successivo        
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Rete Neurale}
    
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large Quando si effettua il conteggio dei livelli di una rete non si considera il livello di input
        \item \large Una rete a \emph{singolo livello} non presenta livelli hidden    
        \item \large Per determinare la grandezza di una rete ci si concentra sul numero di neuroni e sui relativi pesi ad essi associati
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Livello Fully-Connected}
    
    \begin{block}{Definizione} 
        \large Un livello è di tipo \emph{Fully-Connected} quando i neuroni appartenenti a due livelli adiacenti sono completamente connessi tra loro mentre i neuroni associati ad un singolo livello non condividono nessuna connessione 
    \end{block}\pause
    
    \begin{itemize} [<+->]
        \setlength\itemsep{1.5em}
        \item \large I pesi dei neuroni di ciascun livello sono salvati all'interno di matrici
        \item \large Le righe di una matrice identificano i neuroni del livello mentre le colonne contengono i pesi di ciascun neurone
        \item \large La struttura a livelli di una rete neurale permette di sfruttare le potenzialità del calcolo matriciale 
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Livello Fully-Connected}
    
    \begin{center}
      \includegraphics[scale = 0.35]{fully_connected.jpeg}
    \end{center}
  
    \bigskip
  
    \begin{itemize}
        \setlength\itemsep{1em}
        \item[] \large \emph{Una rete neurale a 3 livelli}
    \end{itemize}       
\end{frame} 



\begin{frame}{Reti Neurali}
    \framesubtitle{Funzionamento}
    
    Il processo di apprendimento di una rete neurale è suddiviso in quattro fasi distinte \pause
    
    \bigskip
    \bigskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \emph{\large Inizializzazione dei pesi}
        \item \emph{\large Forward Propagation}
        \item \emph{\large Calcolo della Funzione di Perdita}
        \item \emph{\large Back Propagation}
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Inizializzazione dei pesi}
    
    \smallskip
    
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large Al momento della nascita gli esseri umani non sono in grado di discriminare nessun tipo di oggetto a causa del mancato addestramento della loro rete neurale biologica
        \item \large Per riprodurre questo comportamento, all'inizio della fase di training, i pesi sinaptici $w_i$ di ciascun livello vengono inizializzati in maniera casuale     
    \end{itemize}
\end{frame}



\begin{frame}{Reti Neurali}
    \framesubtitle{Forward Propagation}
    
    \begin{block}{Definizione} 
        \large La \emph{Forward Propagation} è il meccanismo utilizzato da una rete neurale per associare un'immagine ad una determinata classe
    \end{block}\pause
    
    \smallskip
        
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
        \item \large L'output dei neuroni del livello $i$ viene moltiplicato per la matrice dei pesi del livello $i+1$ ottenendo il vettore $v$
        \item \large Al vettore $v$ viene aggiunto il vettore dei bias del livello $i+1$
        \item \large L'output del livello $i+1$ si ottiene applicando la funzione di attivazione $f$ ad ogni entry del vettore $v$
        \item \large Le operazioni precedenti sono svolte per tutti i livelli ad eccezione dell'ultimo
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Calcolo della funzione di perdita}
    
    \begin{block}{Definizione} 
        \large Una \emph{funzione di perdita} $L$ viene utilizzata per determinare l'errore di classificazione di una rete neurale
    \end{block}\pause
        
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
        \item \large La funzione di perdita più usata è la \emph{Mean Squared Error (MSE)} $L = \frac{1}{2} \sum (y - o)^{2}$
        \item \large $y$ identifica l'output della rete mentre $o$ l'etichetta dell'esempio considerato
        \item \large Minimizzando la funzione di perdita $L$ si riduce l'errore di una rete neurale
        \item \large Calcolando la derivata di $L$ in funzione dei pesi $w_i$ si individua il minimo globale della funzione di perdita
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Funzione di perdita}
    
    \begin{center}
      \includegraphics[scale = 0.7]{Loss.png}
    \end{center}
  
    \smallskip
  
    \begin{itemize}
        \setlength\itemsep{1em}
        \item[] \large \emph{Mean Squared Error (MSE). I pesi $w_1$ e $w_2$ sono le variabili indipendenti. La funzione di perdita $L$ è la variabile dipendente}
    \end{itemize}       
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Back Propagation}
    
    \begin{block}{Definizione} 
        \large La \emph{Back Propagation} è il meccanismo utilizzato da una rete neurale per correggere gli errori di classificazione. Vengono individuati i pesi $w_i$ che hanno influenzato maggiormente l'errore commesso e viene aggiornato il loro valore in modo da ridurre la funzione di perdita
    \end{block}\pause
    
    \bigskip 
    
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Per calcolare la derivata della funzione $L$ in funzione dei pesi $w_i$ viene usata la \emph{regola della catena (chain rule)}
        \item \large Questa regola è usata per trovare la derivata di una funzione composta
    \end{itemize}      
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Aggiornamento dei Pesi e Learning Rate}
    
    \begin{itemize} [<+->]
        \setlength\itemsep{1.5em}
        \item \large Il nuovo valore del peso $w_i$ è dato dalla regola di aggiornamento  $w_i = w_i - \eta\frac{\partial L}{\partial w_i} = w_i + \Delta w_i$
        \item \large Il \emph{learning rate} $\eta$ è un parametro usato per controllare la velocità di aggiornamento dei pesi 
        \item \large Un learning rate alto comporta aggiornamenti rapidi, un tempo di esecuzione più basso, ma una maggiore probabilità di terminare in un minimo locale
        \item \large Al contrario, un basso learning rate riduce la probabilità di terminare in un minimo locale, ma i tempi di esecuzione si allungano notevolmente
    \end{itemize}      
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    
    \begin{center}
      \includegraphics[scale = 0.35]{back1.png}
    \end{center}
    
    \begin{columns}
        \begin{column}{0.48\textwidth}
            \begin{equation*}
                z_j^{h} = \sum_{i=0}^{n} w_{ij}^{h}x_i
            \end{equation*}
        \end{column}
        \begin{column}{0.48\textwidth}
            \begin{equation*}
                z^{o} = \sum_{j=0}^{m} w_{j}^{o}h_j
            \end{equation*}
        \end{column}
    \end{columns}
    
    \smallskip
    
    \begin{columns}
        \begin{column}{0.48\textwidth}
            \begin{equation*}
                h_j = f(z_j^{h})
            \end{equation*}
        \end{column}
        \begin{column}{0.48\textwidth}
            \begin{equation*}
                o = f(z^{o})
            \end{equation*}
        \end{column}
    \end{columns}    
\end{frame} 

\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Derivata della funzione $L$ in funzione del peso $w_j^{o}$ 
        
        \bigskip
        \begin{center}
         \large $ \frac{\partial L}{\partial w_j^{o}} =  \frac{\partial L}{\partial o} \cdot \frac{\partial o}{\partial z^{o}} \cdot \frac{\partial z^{o}}{\partial w_j}$         
         \end{center}
         
         \bigskip
         \setbeamertemplate{itemize items}[square]
         \begin{itemize}
            \setlength\itemsep{2.5em}
            \item \large $\frac{\partial L}{\partial o} = \frac{\partial}{\partial o} \big[ \frac{1}{2}(y - o)^{2} \big] = -(y - o)$
            \item \large $\frac{\partial o}{\partial z^{o}} = f^{\prime}(z^{o})$
            \item \large $\frac{\partial z^{o}}{\partial w_j^{o}} = h_j$ 
         \end{itemize}
    \end{itemize}
\end{frame}


\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large Risultato della derivata della funzione $L$ in funzione del peso $w_j^{o}$
        \bigskip
        \begin{center}
         \large $ \frac{\partial L}{\partial w_j^{o}} =  -(y - o) \cdot f^{\prime}(z^{o}) \cdot h_j = -\delta_j^{o}h_j$         
         \end{center}
         \item \large Aggiornamento del peso $w_j^{o}$
         \bigskip
         \begin{center}
         \large $ \Delta w_j^{o} = \eta\delta_j^{o}h_j$         
         \end{center}       
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    
    \begin{center}
      \includegraphics[scale = 0.4]{back2.png}
    \end{center}
    
    \bigskip  

     \begin{equation*}
         \frac{\partial L}{\partial w_{ij}^{h}} =  \frac{\partial L}{\partial o} \cdot \frac{\partial o}{\partial z^{o}} \cdot \frac{\partial z^{o}}{\partial h_j} \cdot \frac{\partial h_j}{\partial z_j^{h}} \cdot \frac{\partial z_j^{h}}{\partial w_{ij}^{h}}
     \end{equation*}            
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        
        \item[] \large \begin{center} $\frac{\partial L}{\partial o} = \frac{\partial}{\partial o} \big[ \frac{1}{2}(y - o)^{2} \big] = -(y - o)$ \end{center}
        \item[] \large \begin{center} $\frac{\partial o}{\partial z^{o}} = f^{\prime}(z^{o})$ \end{center}
        \item[] \large \begin{center} $\frac{\partial z^{o}}{\partial h_j} = w_j^{o}$ \end{center}
        \item[] \large \begin{center} $\frac{\partial h_j}{\partial z_j^{h}} = f^{\prime}(z_j^{h})$ \end{center}
        \item[] \large \begin{center} $\frac{\partial z_j^{h}}{\partial w_{ij}^{h}} = x_i$ \end{center}          
    \end{itemize}
\end{frame}


\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Back Propagation}
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large Risultato della derivata della funzione $L$ in funzione del peso $w_{ij}^{h}$
        \bigskip 
        \begin{center}
         \large $ \frac{\partial L}{\partial w_{ij}^{h}} =  -(y - o) \cdot f^{\prime}(z^{o}) \cdot w_j^{o} \cdot f^{\prime}(z_j^{h}) \cdot x_i = -\delta_j^{h}x_i$         
         \end{center}
         \item \large Aggiornamento del peso $w_{ij}^{h}$
         \bigskip
         \begin{center}
         \large $ \Delta w_{ij}^{h} = \eta\delta_j^{h}x_i$         
         \end{center}       
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Rete Neurale Convoluzionale}
    
    \begin{block}{Definizione}  
    Una \emph{Rete Neurale Convoluzionale} si differenzia da una rete neurale classica per la presenza di un nuovo tipo di livello: il \emph{livello Convoluzionale}
    \end{block}\pause
    
    \begin{itemize} [<+->]
        \setlength\itemsep{1.5em}
        \item \large Un livello convoluzionale è composto da diversi \emph{filtri}
        \item \large Ogni filtro ricerca all'interno delle immagini della rete una o più \emph{feature}: linee, curve, pattern
        \item \large La \emph{profondità (depth)} di un livello è pari dal numero di filtri che lo compongono
        \item \large I filtri equivalgono alle matrici dei pesi dei livelli fully-connected e la loro dimensione è inferiore all'output del livello precendente
    \end{itemize}    
\end{frame}


\begin{frame}{Reti Neurali}
    \framesubtitle{Livello Convoluzionale}  
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large La rete deve individuare feature sempre più complesse per apprendere nel miglior modo possibile il contenuto semantico di un'immagine
        \item \large Le feature più complesse si ottengono mettendo in sequenza più livelli convoluzionali tra loro
        \item \large L'output di un generico livello convoluzionale $i$ diventa l'input del successivo livello $i+1$. Le feature prodotte da $i$ sono meno complesse di quelle ottenute da $i+1$
    \end{itemize}   
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Funzionamento}
    
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large I pesi dei filtri di un livello convoluzionale sono inizializzati in maniera casuale
        \item \large Vengono utilizzate le stesse funzioni di attivazione e le stesse funzioni di perdita dei livelli fully-connected
        \item \large La forward e la back propagation sono le uniche fasi che subiscono modifiche     
    \end{itemize}
\end{frame}


\begin{frame}{Reti Neurali}
    \framesubtitle{Forward Propagation} 
    
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
        \item \large All'inizio della forward il filtro viene sovrapposto alla parte superiore sinistra della matrice di input
        \item \large Si effettua la \emph{convoluzione} tra le due sottomatrici e il risultato ottenuto viene salvato nella matrice di output 
        \item \large Il filtro viene spostato di una posizione verso destra e viene rieseguita nuovamente la convoluzione  
        \item \large Al termine della riga il filtro viene risposizionato nella parte sinistra della matrice di input, ma spostato in basso di una riga
        \item \large Gli ultimi due passaggi vengono ripetuti fino a riempire tutta la matrice di output  
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Esempio Forward Propagation} 
    
    \begin{center}
      \includegraphics[scale = 0.7]{forward_conv.png}
    \end{center}
  
    \smallskip
    
    \begin{itemize}
        \setlength\itemsep{1em}
        \item[] \large \emph{Esempio di Forward Propagation. La matrice di input è un $7 \times 7$. Il filtro è un $3 \times 3$ e l'output è un $5 \times 5$}
    \end{itemize}
\end{frame}

\begin{frame}{Reti Neurali}
    \framesubtitle{Considerazioni Forward Propagation} 
    
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large La dimensione della matrice di output viene ricavata da questa formula $O = (W - K) + 1$
        \item \large $O$ rappresenta l'altezza e la larghezza della matrice di output, $W$ quella della matrice di input e $K$ la dimensione del filtro
        %\item \large La matrice di output risultante è più piccola rispetto a quella di input
        %\item \large Una sequenza di diversi livelli convoluzionali comporta la perdita di una grande quantità di informazione spaziale
        %\item \large Il \emph{padding} è il meccanismo per mezzo del quale è possibile avere una matrice di output della stessa dimensione di quella di input 
    \end{itemize}
\end{frame}

\begin{comment}
\begin{frame}{Reti Neurali}
    \framesubtitle{Padding} 
    
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
        \item \large Il padding si ottiene aggiungendo un certo numero di zeri in cima ed in fondo alle righe e alle colonne della matrice di input
        \item \large L'operazione viene fatta prima che effettuata la convoluzione con i vari filtri del livello
        \item \large Il numero di zeri da aggiungere è dato da $P = \frac{(K - 1)}{2}$
        \item \large La dimensione della matrice di output è uguale a
         \bigskip            
         \begin{center} 
             $O = (W - K + 2P) + 1$ \\
             \bigskip
             $O = (W - K + 2 \cdot \frac{(K - 1)}{2}) + 1$ \\
             \bigskip
             $O = W$ 
         \end{center}
                     
    \end{itemize}
\end{frame}
\end{comment}

\begin{frame}{Reti Neurali}
    \framesubtitle{Back Propagation}  
    Una \emph{Rete Neurale Convoluzionale} si differenzia da una più classica in quanto assume che l'input della rete sia un'immagine
\end{frame}



   

\begin{frame}[c]
  \centering
  \bigskip \bigskip    
  \Huge Implementazione della Rete
\end{frame}

\begin{frame}{Implementazione della Rete}
    \framesubtitle{Obiettivo}
    \begin{itemize} [<+->]
        \setlength\itemsep{2em}
        \item \large Si vuole costruire una rete neurale convoluzionale che permetta il riconoscimento di cifre numeriche scritte a mano
        \item \large Le cifre da riconoscere sono salvate come immagini in scala di grigio a 8 bit. Un pixel può assumere solo i valori che sono compresi nell'intervallo $[0,255]$
        \item \large L'output della rete è dato dalle 10 cifre numeriche che si vogliono riconoscere
        \item \large La rete riceve in input un'immagine e le associa la cifra numerica corrispondente
    \end{itemize}
\end{frame}


\begin{frame}{Implementazione della Rete}
    \framesubtitle{Dati}
    \begin{itemize} [<+->]
        \setlength\itemsep{3em}
        \item \large Le immagini che identificano gli esempi del training e del test set hanno una dimensione di $28 \times 28$ mentre le etichette rappresentano le cifre corrispondenti alle immagini
        \item \large Il training ed il test set provengono dal database \emph{MNIST} e contengono rispettativamente $60000$ esempi di train e $10000$ di test 
    \end{itemize}
\end{frame}


\begin{frame}{Implementazione della Rete}
    \framesubtitle{Struttura}
    \begin{itemize} [<+->]
        \setlength\itemsep{2.5em}
        \item \large La rete neurale convoluzionale sviluppata si compone di 3 livelli
        \item \large Due livelli hidden di tipo convoluzionale ed un livello di output di tipo fully-connected
        \item \large La struttura si basa su una rete neurale convoluzionale chiamata \emph{Dnn} 
        \item \large La Dnn è scritta in linguaggio C e adotta un approccio di tipo sequenziale
    \end{itemize}
\end{frame}

\begin{frame}{Implementazione della Rete}
    \framesubtitle{Struttura}
    
    \begin{table}
        \centering
        \begin{tabular}{| c | c | c | c | c |}
           \hline
           & Input & Hidden 1 & Hidden 2 & Output \\
           \hline
           \emph{Dimensione} & $28 \times 28$ & $24 \times 24$ & $20 \times 20$ & $10 \times 1$ \\ 
           \hline         
           \emph{Numero di Nodi} & 784 & 2880 & 2000 & 10 \\
           \hline
           \emph{Profondità} & 1 & 1 & 1 & 1 \\
           \hline
           \emph{Dimensione filtro} & & 5 & 5 & \\
           \hline
        \end{tabular}
    \caption{Struttura Rete Neurale}
    \end{table} 
    
    \begin{table}
        \centering
        \begin{tabular}{| c | c | c | c | c |}
           \hline
           & Input & Hidden 1 & Hidden 2 & Output \\
           \hline
           \emph{Sigmoide} & & \checkmark & \checkmark & \checkmark \\ 
           \hline         
           \emph{Tanh} & & \checkmark & \checkmark & \checkmark \\
           \hline
           \emph{Softplus} & & \checkmark & \checkmark & \checkmark \\
           \hline          
        \end{tabular}
    \caption{Funzioni di attivazione per livello}
    \end{table} 
     
    
\end{frame}


\begin{frame}{Implementazione della Rete}
    \framesubtitle{Considerazioni}
    \begin{itemize} [<+->]
        \setlength\itemsep{1em}
        \item \large I calcoli interni alla rete sono svolti usando il formato di dato \emph{double} in modo da non perdere precisione numerica nei vari passaggi
        \item \large All'inizio della fase di training i pixel delle immagini vengono riscalati nell'intervallo $[0,1]$ per poter essere compatibili con il formato di dato usato dalla rete
        \item \large Tutti i dati e le strutture dati necessarie al funzionamento della rete vengono allocate all'inizio dell'esecuzione e deallocate al suo termine
        \item \large In modo da poter confrontare tra loro i risultati ottenuti le funzioni di attivazione utilizzate sono le stesse della rete sequenziale
    \end{itemize} 
    
\end{frame}



\begin{frame}[c]
  \centering
  \bigskip \bigskip    
  \Huge Analisi dei Risultati
\end{frame}


\end{document}
